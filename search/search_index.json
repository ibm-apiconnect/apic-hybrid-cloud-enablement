{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#ibm-hybrid-cloud-integration","title":"IBM Hybrid Cloud Integration","text":""},{"location":"#cp4i-ibm-api-connect-datapower","title":"CP4I / IBM API Connect / DataPower","text":"<p>You can use IBM Integration products independently or in combination. This repository contains short documents which illustrate product features and solutions to real world use cases. </p> <ul> <li>IBM Cloud Pak for Integration (CP4I)  The CP4I family comprises IBM AppConnect, MQ, Event Streams, API Connect, DataPower and Aspera. The current documents are mainly about IBM API Connect &amp; DataPower.  </li> <li>IBM API Connect (APIC) </li> <li>IBM DataPower Gateway </li> </ul>"},{"location":"#ibm-concert","title":"IBM Concert","text":"<ul> <li>OpenShift Deployment</li> <li>Kubernetes Deployment</li> </ul> <p>The code and artifacts in this repository are made available under the GNU Public License v2. This repository is provided 'AS-IS' with NO guarantee of official IBM support or fitness for purpose.</p>"},{"location":"IBM%20Concert/overview/","title":"Overview","text":"<p>Sila Kissuu  \u00a9 IBM v0.1  2025-01-31   </p>"},{"location":"architecture/APIC-HA-Architectures/","title":"High Availability with Two Data Centers","text":""},{"location":"architecture/APIC-HA-Architectures/#ibm-api-connect-architecture","title":"IBM API Connect: Architecture","text":"<p>Ravi Ramnarayan   \u00a9 IBM v1.81  2023-10-19   </p>"},{"location":"architecture/APIC-HA-Architectures/#goals","title":"Goals","text":"<ul> <li>Compare High Availability (HA) architectures for IBM API Connect v10 (APIC) on OpenShift   <ul> <li>Two Data Center Deployment (2DCDR) </li> <li>APIC with DataPower HA in DC2</li> </ul> </li> <li>Outline steps to deploy APIC with DataPower HA in DC2 </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#prologue","title":"Prologue","text":"<p>The data centers could be on premises or vendor sites. Ideally, the data centers should be in different locations with separate lines for power and communication.   </p> <p>Tabled for a later discussion Modern cloud vendors can provide a variation which might be less expensive. APIC subsystems comprise three pods in Production deployments. Within a single OCP cluster, each subsystem pod could run in different regions (vendor nomenclature vary), providing a quorum of two active pods even if a region failed.  </p>"},{"location":"architecture/APIC-HA-Architectures/#apic-deployment-architecture","title":"APIC Deployment Architecture","text":"<p>We compare two APIC deployment architectures using the metrics Recovery Time Objective (RTO) and Recovery Point Objective (RPO). Please see Planning your deployment topology.   </p> <p>Recovery Point Objective (RPO)  The RPO measures how far back in time the recovery point is, and therefore how much new data is lost. An RPO of zero would assert that no data is lost, but such a solution is often a compromise against the cost and performance of the system.</p>"},{"location":"architecture/APIC-HA-Architectures/#1-two-data-center-deployment-2dcdr","title":"1. Two Data Center Deployment (2DCDR)","text":""},{"location":"architecture/APIC-HA-Architectures/#pro","title":"Pro","text":"<ul> <li>High Availability (HA) for API traffic   With adequate capacity, the DataPower cluster in a single DC could hand the full load if the other DC fails.  </li> <li>RPO for API Products   Near zero RPO for published API Products.  </li> <li>RPO for Consumer Subscriptions   Near zero RPO for Consumer Subscriptions. If the business requires near zero RPO, the Two data center deployment strategy on Kubernetes and OpenShift is the appropriate choice.  </li> <li>RTO Failover    If the active APIC cluster fails, it is possible to activate APIC on the other cluster in a short period. APIC functions cannot be performed until the completion of RTO Failover.   <ul> <li>Consumer Organizations cannot initiate new Subscriptions   </li> <li>Provider Organizations cannot publish or update API Products   </li> </ul> </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#contra","title":"Contra","text":"<ul> <li>Maintenance   Typically, APIC upgrades occur two to four times a year. Each upgrade bears the burden of increased time and effort.       <ul> <li>Maintaining a two data center deployment is complex  </li> <li>Upgrading a two data center deployment specifies operations on both OCP clusters within the same change window    </li> </ul> </li> <li>RTO to restore 2DCDR requires Disaster Recovery (DR)      The goal of DR is to restore 2DCDR, which is different from RTO Failover. DR processes need to be aware of the active APIC site, adding complexity.       </li> <li>Costs   Software license and operational costs might be higher than the other architectures. Please verify specifics for your installations.  </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#2-apic-with-datapower-ha-in-dc2","title":"2. APIC with DataPower HA in DC2","text":""},{"location":"architecture/APIC-HA-Architectures/#pro_1","title":"Pro","text":"<ul> <li>High Availability (HA) for API traffic   Same HA level as the 2DCDR architecture.  </li> <li>RPO for API Products   It is possible to achieve near zero RPO tolerance with a backup soon after publishing products.  </li> <li>Maintenance   <ul> <li>Upgrade of APIC &amp; DataPower in DC1 using the top level APIConnectCluster Custom Resource (CR) requires less effort than upgrading APIC subsystems of the 2DCDR deployment</li> <li>DataPower subsystem in DC2 can be upgraded in a separate change window   Starting with APIC v10.0.5.x, all fixpacks (fourth position) of APIC will be compatible with DataPower v10.5.0.x. The flexibility allows a relaxed schedule to upgrade DC2.     </li> </ul> </li> <li>Disaster Recovery (DR)    The process is less complex than DR for 2DCDR and should to be quicker to complete.   </li> <li>Costs   Software license and operational costs might be lower than for 2DCDR. Please verify specifics for your installations.  </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#contra_1","title":"Contra","text":"<ul> <li>RPO for Consumer Subscriptions depends on the frequency of backups   It is possible to capture backups frequently, but RPO will not be comparable to 2DCDR's near zero tolerance for Consumer Subscriptions.    </li> <li>RTO depends on DR   The recovery time can be reduced with automation and DR drills. However, it is likely to take longer than the RTO Failover enabled by 2DCDR. APIC functions cannot be performed until DR restores normal service.   <ul> <li>Consumer Organizations cannot initiate new Subscriptions   </li> <li>Provider Organizations cannot publish or update API Products  </li> </ul> </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#deploy-apic-with-datapower-ha-in-dc2","title":"Deploy APIC with DataPower HA in DC2","text":"<p>Note: The steps below assume a connected OCP. Adjust as needed for air-gapped OCP.     </p> <ul> <li>Connected OCP   Use Installing operators </li> <li>Air-gapped OCP   Use the appropriate recipe in Air-gapped installation.  </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#install-apic-in-dc1","title":"Install APIC in DC1","text":"<p>Start with Installing API Connect and follow the trail to Installing with the top-level CR on OpenShift.</p>"},{"location":"architecture/APIC-HA-Architectures/#install-datapower-in-dc2","title":"Install DataPower in DC2","text":"<p>Installing with subsystem CRs in different namespaces or environments provides the overview. We will install the DataPower subsystem in DC2.    </p> <ul> <li>OCP in DC2 needs preparations such as <code>pull-secret</code> and the IBM Operator Catalog source.</li> <li>Install the IBM API Connect operator in <code>apigw2</code> namespace in DC2      This is the same operator used to install APIC in DC1.  </li> <li>The versions of the operators in DC1 &amp; DC2 must be the same      </li> <li>Make sure the DC2 cluster has the same certificates as APIC in DC1     See Common Issuers &amp; Gateway Secrets.  </li> </ul> <p>Note: DataPower in DC2 could be on OCP or Kubernetes (K8s). This document details steps for DataPower on OCP. VMware and Linux are feasible platforms, but DataPower configuration and maintenance operations are more intense than OCP/K8s.</p>"},{"location":"architecture/APIC-HA-Architectures/#namespace-operators","title":"Namespace &amp; Operators","text":"<ul> <li>Create a namespace, for example, <code>apigw2</code> <p>Note: Follow your naming conventions. <code>apigw2</code> is just an example.  </p> </li> <li>Install the IBM API Connect operator in <code>apigw2</code> <ul> <li>Subscribe to the same channel as the parent APIC in DC1    </li> </ul> </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#common-issuers-gateway-secrets","title":"Common Issuers &amp; Gateway Secrets","text":"<p>Follow steps in Installing the Gateway subsystem section Before you begin.    </p> <ul> <li> <p>Clone <code>ingress-ca</code> from the primary OCP in DC1     Installing the Gateway subsystem section Before you begin Step 1. Detailed instructions are in Extracting the Management ingress-ca certificates.  </p> <ul> <li>Apply the extracted <code>ingress-ca</code> to the DC2 Project <code>apigw2</code> <code>oc apply -f &lt;your ingress-ca extract.yaml&gt; -n apigw2</code> </li> <li>Get the complete name of <code>ingress-ca</code> <pre><code>oc get secret | grep ingress-ca    \napis-minimum-ingress-ca            kubernetes.io/tls         3      16m    \n</code></pre> <p>Note: The secret name in your installation will be different.  </p> </li> </ul> </li> <li> <p>Define Common issuers and Gateway secrets    Obtain YAML from Installing the Gateway subsystem section Before you begin Step 2.</p> <ul> <li> <p>Edit the YAML and set the Issuer <code>spec.ca.secretName</code> to value above See example 230-common-issuer-and-gateway-certs.yaml which sets <code>spec.ca.secretName</code> to <code>apis-minimum-ingress-ca</code>. <pre><code>apiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: ingress-issuer\n  labels: {\n    app.kubernetes.io/instance: \"management\",\n    app.kubernetes.io/managed-by: \"ibm-apiconnect\",\n    app.kubernetes.io/name: \"ingress-issuer\"\n  }\nspec:\n  ca:\n    secretName: apis-minimum-ingress-ca\n</code></pre></p> </li> <li> <p>Apply your YAML to Project <code>apigw2</code> in DC2   <pre><code>oc apply -f &lt;your-common-issuer-and-gateway-certs.yaml&gt; -n apigw2  \n</code></pre></p> </li> <li> <p>Confirm issuers were created and ready for use <pre><code>oc get issuers -n apigw2    \nNAME                 READY   AGE    \ningress-issuer       True    13m    \nselfsigning-issuer   True    13m    \n</code></pre></p> </li> <li> <p>Confirm the creation of gateway secrets <pre><code>oc get secrets -n apigw2 | grep gateway    \ngateway-peering                    kubernetes.io/tls         3      24s  \ngateway-service                    kubernetes.io/tls         3      24s  \n</code></pre></p> <p>Note: \"3\" in the third column indicates three components in the TLS (good).  </p> </li> </ul> </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#deploy-datapower-gateway-in-dc2","title":"Deploy DataPower Gateway in DC2","text":"<ul> <li> <p>Create Gateway admin secret    You could use the same password as in the DC1 OCP or assign a different value. Create the secret in DC2 namespace <code>apigw2</code>: <pre><code>oc -n apigw2 create secret generic &lt;gw_admin_secret_name&gt; \\\n  --from-literal=password=&lt;gw_admin_pswd&gt;  \n</code></pre></p> </li> <li> <p>Define the APIGW Gateway     Copy the YAML from Installing the Gateway subsystem section Procedure Step 2. Use values appropriate to your installation. Recommend following naming conventions in DC1.    </p> <ul> <li>$ fields  </li> <li><code>metadata.name</code> (optional, default value is okay)  </li> <li><code>metadata.labels.app.kubernetes.io/name</code> (optional, default value is okay) </li> <li><code>spec.adminUser.secretName</code> should be <code>&lt;gw_admin_secret_name&gt;</code>, which you created in the previous step    </li> </ul> </li> </ul> <p>See Example Values in 250-apigateway_cr.yaml.  </p> <p>Note:  Example values are NOT shell script substitutions. You should edit the file manually. <pre><code>oc apply -f &lt;your-apigateway_cr.yaml&gt; -n apigw2`  \n</code></pre>   It takes about five minutes to create the GatewayCluster.     </p> <ul> <li> <p>Is the gateway running?   <code>oc get GatewayCluster -n apigw2</code> </p> </li> <li> <p>Is the Gateway Management endpoint active? <code>curl -k https://&lt;spec.gatewayManagerEndpoint.hosts.name&gt;/health</code>   should return: <code>{\"status\":\"ok\"}</code> </p> </li> <li> <p>Register DC2 Gateway Service in the parent DC1 APIC   Obtain URL for endpoints from OCP Routes in project <code>apigw2</code>.</p> </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#deploy-analytics-in-dc2","title":"Deploy Analytics in DC2","text":"<p>Note: Optional. Analytics is not needed in DC2, unless the API traffic is extremely high. You could install Analytics at a later date, if needed.  </p> <p>Deploy the Analytics subsystem in DC2 within the same namespace as the DataPower Gateway. The steps in Installing the Analytics subsystem are similar to Installing the Gateway subsystem.   </p> <ul> <li> <p>Common Issuers   Nothing to do. Been there, done that in Common Issuers &amp; Gateway Secrets. Skip over section Before you begin in Installing the Analytics subsystem.    </p> </li> <li> <p>Obtain YAML from section Procedure  Step 1 of Installing the Analytics subsystem.   </p> </li> <li> <p>Ensure <code>clientSubjectDN</code> in DC2 Analytics matches the  Subject common name of DC1 APIC CMC Analytics ingestion keystore.</p> <ul> <li>In APIC 10.0.5.1, the Analytics ingestion keystore Subject common name is <code>a7s-ing-client</code> </li> <li>Modify <code>spec.ingestion.clientSubjectDN</code> to match The sample file 280-analytics_cr.yaml  contains the correction.  </li> </ul> </li> <li> <p>Create the Analytics subsystem in DC2 <code>oc apply -f &lt;your-analytics_cr.yaml&gt; -n apigw2</code> </p> </li> <li> <p>Is the Analytics subsystem running?   <code>oc get AnalyticsCluster -n apigw2</code> </p> </li> <li> <p>Register DC2 Analytics in the parent DC1 APIC   </p> </li> <li>Associate DC2 Analtyics with the DC2 DataPower Service    </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#troubleshoot","title":"Troubleshoot","text":"<p>If you encounter a <code>403 Forbidden</code> error while registering the Analytics Service, the cause is likely to be a mismatch of <code>clientSubjectDN</code>.  </p> <ul> <li> <p>Find the pod named <code>mtls-gw</code> <pre><code>$ oc get pod -n apigw2 | grep mtls\nanalytics-mtls-gw-6bbc4cbfcc-gchhc                       1/1     Running     0             17m\n</code></pre></p> </li> <li> <p>Do the logs show mismatched <code>clientSubjectDN</code>?  <pre><code>  $ oc logs analytics-mtls-gw-6bbc4cbfcc-gchhc | grep 403 -A2 -B1  \n</code></pre></p> <pre><code>  10.254.20.1 - - [05/Dec/2022:23:08:36 +0000] \"GET /_healthz HTTP/1.1\" 200 2 \"-\" \"kube-probe/1.23\"  \n  2022/12/05 23:08:36 [warn] 12#12: *2106 [lua] access_by_lua(nginx.conf:56):7: rejected request because certificate subject 'CN=a7s-ing-client' did not match expected 'CN=a7s-ing-client,O=cert-manager', client: 10.254.12.1, server: ai.apps.fainted.myco.com, request: \"POST /analytics-service-registration-create?admin=true&amp;management=true HTTP/1.1\", host: \"ai.apps.fainted.myco.com\"  \n  10.254.12.1 - - [05/Dec/2022:23:08:36 +0000] \"POST /analytics-service-registration-create?admin=true&amp;management=true HTTP/1.1\" 403 159 \"-\" \"axios/0.26.1\"  \n  10.254.20.1 - - [05/Dec/2022:23:08:46 +0000] \"GET /_healthz HTTP/1.1\" 200 2 \"-\" \"kube-probe/1.23\"  \n</code></pre> </li> <li> <p>Fix the problem     Modify the <code>clientSubjectDN</code> in the DC2 AnalyticsCluster YAML.   </p> </li> </ul>"},{"location":"architecture/APIC-HA-Architectures/#references","title":"References","text":"<ul> <li>Certificate lists for Cloud Pak for Integration </li> <li>List of Issuers, CA certificates, and secrets </li> </ul>"},{"location":"datapower/DP-LogTarget-no-WebGUI/","title":"DataPower Log Target without WebGUI - Example","text":""},{"location":"datapower/DP-LogTarget-no-WebGUI/#ibm-datapower-migrate-to-cloud","title":"IBM DataPower: Migrate to Cloud","text":"<p>Ravi Ramnarayan  \u00a9 IBM v1.26  2022-10-13  </p>"},{"location":"datapower/DP-LogTarget-no-WebGUI/#prolog","title":"Prolog","text":"<ul> <li>This document is a bare bones example. Please refer to DataPower Config without WebGUI for context and explanations.  </li> <li>The IBM Techcon 2021 session Re-imagining DataPower in the container world explains steps to develop DataPower assemblies and configurations on your workstation. You might have to register to access the recording or download slides. </li> </ul>"},{"location":"datapower/DP-LogTarget-no-WebGUI/#steps","title":"Steps","text":"<ul> <li>Setup DataPower on Docker </li> <li>Create Log Target using WebGUI</li> <li>Isolate Log Target <code>config</code> statements </li> <li>Define k8s/OCP ConfigMap &amp; Inject DataPower Log Target   Inject Log Target <code>config</code> into operator Custom Resource (CR) YAML.  <ul> <li>k8s GatewayCluster CR  </li> <li>OCP APIConnectCluster CR  </li> </ul> </li> </ul>"},{"location":"datapower/DP-LogTarget-no-WebGUI/#setup-datapower-on-docker","title":"Setup DataPower on Docker","text":""},{"location":"datapower/DP-LogTarget-no-WebGUI/#ubuntu-20-vm-ubu20","title":"Ubuntu 20 VM <code>ubu20</code>","text":"<ul> <li>Install Docker  </li> <li> <p>Set up a local working environment <pre><code>rramnara@ubu20:~$ mkdir dp-dev &amp;&amp; cd dp-dev\n# setup directories for volume mounts\nrramnara@ubu20:~/dp-dev$ mkdir config local certs\nrramnara@ubu20:~/dp-dev$ chmod 777 config local certs\nrramnara@ubu20:~/dp-dev$ ll\ntotal 20\ndrwxrwxr-x 5 rramnara rramnara 4096 Mar 15 20:19 ./\ndrwxr-xr-x 9 rramnara rramnara 4096 Mar 15 20:37 ../\ndrwxrwxrwx 2 rramnara rramnara 4096 Mar 15 20:19 certs/\ndrwxrwxrwx 2 rramnara rramnara 4096 Mar 15 20:20 config/\ndrwxrwxrwx 2 rramnara rramnara 4096 Mar 15 20:20 local/\n</code></pre></p> </li> <li> <p>Start a container, mounting relevant directories (ex. config, local, certs)  </p> <p>Note: <code>docker run</code> command contains <code>-p 9090:9090</code> which is not in Re-imagining DataPower in the container world slide 9.</p> <pre><code>docker run -it \\\n-e DATAPOWER_ACCEPT_LICENSE=true \\\n-e DATAPOWER_INTERACTIVE=true \\\n-p 9090:9090 \\\n-v $(pwd)/config:/opt/ibm/datapower/drouter/config \\\n-v $(pwd)/local:/opt/ibm/datapower/drouter/local \\\n-v $(pwd)/certs:/opt/ibm/datapower/root/secure/usrcerts \\\n--name dp-dev \\\nicr.io/integration/datapower/datapower-limited:10.0.3.0\n</code></pre> <p>Produces about 30 lines of output. The last couple of lines should resemble:</p> <pre><code>20220315T203900.823Z [0x8100003b][mgmt][notice] domain(default): Domain configured successfully.\n20220315T203901.071Z [0x00350014][mgmt][notice] quota-enforcement-server(QuotaEnforcementServer): tid(831): Operational state up\n</code></pre> </li> <li> <p>Access the container\u2019s CLI, login with \u2018admin\u2019 user (default pw: admin) <pre><code>login: admin\nPassword: *****\n\nWelcome to IBM DataPower Gateway console configuration.\nCopyright IBM Corporation 1999, 2021\n\nVersion: IDG.10.0.3.0 build 333705 on Jun 16, 2021 9:06:57 PM\nDelivery type: CD\nSerial number: 0000001\n</code></pre></p> </li> <li> <p>Enable WebGUI <pre><code>idg# co\nGlobal mode\nidg(config)# write mem\nOverwrite previously saved configuration? Yes/No [y/n]: y\n20220315T203939.267Z [0x8100000c][mgmt][notice] : tid(7174): Saved current configuration to 'config:///auto-startup.cfg'\n20220315T203939.271Z [0x81000040][mgmt][notice] domain(default): tid(7174): Domain configuration has been saved.\n20220315T203939.304Z [0x8100000c][mgmt][notice] : tid(111): Saved current configuration to 'config:///auto-user.cfg'\nConfiguration saved successfully.\nidg(config)# web-mgmt; admin enabled; exit\nModify Web Management Service configuration\n\nidg(config)# 20220315T203958.606Z [0x8100003f][mgmt][notice] domain(default): tid(303): Domain configuration has been modified.\n20220315T203958.609Z [0x00350014][mgmt][notice] web-mgmt(WebGUI-Settings): tid(303): Operational state up\n\nidg(config)# write mem\nOverwrite previously saved configuration? Yes/No [y/n]: y\nConfiguration saved successfully.\nidg(config)# 20220315T204007.739Z [0x8100000c][mgmt][notice] : tid(7174): Saved current configuration to 'config:///auto-startup.cfg'\n20220315T204007.743Z [0x81000040][mgmt][notice] domain(default): tid(7174): Domain configuration has been saved.\n20220315T204007.761Z [0x8100000c][mgmt][notice] : tid(111): Saved current configuration to 'config:///auto-user.cfg'\n\nidg(config)# show web-mgmt\n\nweb-mgmt [up]\n--------\n admin-state enabled\n ip-address 0.0.0.0\n port 9090\n save-config-overwrite on\n idle-timeout 600 Seconds\n acl web-mgmt  [up]\n ssl-config-type server\n enable-sts on\n\nidg(config)# exit\nidg# exit\nGoodbye.\n59ab55cf2bb1\nUnauthorized access prohibited.\nlogin:\n</code></pre></p> <p>Note: Enter Ctrl-PQ to disengage from DataPower without killing the process.  </p> </li> </ul>"},{"location":"datapower/DP-LogTarget-no-WebGUI/#linux-workstation","title":"Linux workstation","text":"<ul> <li>Download DataPower files to workstation <pre><code>rramnara:DP-LogTarget$ mkdir dp-dev-0\nrramnara:DP-LogTarget$ cd dp-dev-0/\nrramnara:dp-dev-0$ scp -r rramnara@192.168.122.60:/home/rramnara/dp-dev/* ./\nwebgui-privkey.pem                                   100% 1704   968.1KB/s   00:00\nwebgui-sscert.pem                                    100% 1147     1.1MB/s   00:00\ndefault.cfg                                          100%   95   125.1KB/s   00:00\nauto-startup.cfg                                     100%   20KB  19.1MB/s   00:00\nauto-user.cfg                                        100%  293   158.1KB/s   00:00\nChrystoki.conf                                       100% 1440     3.0MB/s   00:00\nrramnara:dp-dev-0$ ll\ntotal 12\ndrwxrwxr-x. 2 rramnara rramnara 4096 Mar 15 16:44 certs\ndrwxrwxr-x. 2 rramnara rramnara 4096 Mar 15 16:44 config\ndrwxrwxr-x. 3 rramnara rramnara 4096 Mar 15 16:44 local\nrramnara:dp-dev-0$ tree -L 3\n.\n\u251c\u2500\u2500 certs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 webgui-privkey.pem\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 webgui-sscert.pem\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 auto-startup.cfg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 auto-user.cfg\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 default.cfg\n\u2514\u2500\u2500 local\n    \u2514\u2500\u2500 luna_config\n        \u251c\u2500\u2500 Chrystoki.conf\n        \u251c\u2500\u2500 configData\n        \u2514\u2500\u2500 data\n\n6 directories, 6 files\n</code></pre></li> </ul>"},{"location":"datapower/DP-LogTarget-no-WebGUI/#create-log-target-using-webgui","title":"Create Log Target using WebGUI","text":""},{"location":"datapower/DP-LogTarget-no-WebGUI/#datapower-webgui","title":"DataPower WebGUI","text":"<ul> <li> <p>Define Log Target    </p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> </li> </ul>"},{"location":"datapower/DP-LogTarget-no-WebGUI/#linux-workstation_1","title":"Linux workstation","text":"<ul> <li>Download DataPower files to workstation  <pre><code>rramnara:DP-LogTarget$ mkdir dp-dev-2\nrramnara:DP-LogTarget$ cd dp-dev-2/\nrramnara:dp-dev-2$ scp -r rramnara@192.168.122.60:/home/rramnara/dp-dev/* ./\nwebgui-privkey.pem                                   100% 1704     2.4MB/s   00:00\nwebgui-sscert.pem                                    100% 1147     1.4MB/s   00:00\ndefault.cfg                                          100%   95   183.2KB/s   00:00\nauto-startup.cfg                                     100%   20KB  24.0MB/s   00:00\nauto-user.cfg                                        100%  293   517.4KB/s   00:00\nChrystoki.conf                                       100% 1440     2.2MB/s   00:00\nrramnara:dp-dev-2$ ll\ntotal 12\ndrwxrwxr-x. 3 rramnara rramnara 4096 Mar 16 10:06 certs\ndrwxrwxr-x. 2 rramnara rramnara 4096 Mar 16 10:06 config\ndrwxrwxr-x. 3 rramnara rramnara 4096 Mar 16 10:06 local\nrramnara:dp-dev-2$ tree -L 3\n.\n\u251c\u2500\u2500 certs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 luna_cert\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 webgui-privkey.pem\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 webgui-sscert.pem\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 auto-startup.cfg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 auto-user.cfg\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 default.cfg\n\u2514\u2500\u2500 local\n    \u2514\u2500\u2500 luna_config\n        \u251c\u2500\u2500 Chrystoki.conf\n        \u251c\u2500\u2500 configData\n        \u2514\u2500\u2500 data\n\n7 directories, 6 files\n</code></pre></li> </ul>"},{"location":"datapower/DP-LogTarget-no-WebGUI/#isolate-log-target-config-statements","title":"Isolate Log Target <code>config</code> statements","text":""},{"location":"datapower/DP-LogTarget-no-WebGUI/#linux-workstation_2","title":"Linux workstation","text":"<ul> <li> <p>Isolate <code>config</code> changes   Compare <code>auto-startup.cfg</code> files.   <pre><code>rramnara:DP-LogTarget$ diff dp-dev-0/config/auto-startup.cfg dp-dev-2/config/auto-startup.cfg\n3c3\n&lt; # configuration fcdbe397c2b44f5fa3828d06078ba297 generated Tue Mar 15 16:40:07 2022; firmware version 333705\n---\n&gt; # configuration 01552a3f680641409c738d06078b0ee1 generated Wed Mar 16 09:55:05 2022; firmware version 333705\n179a180,214\n&gt;\n&gt; logging target \"filebeat\"\n&gt;   summary \"filebeat\"\n&gt;   type syslog\n&gt;   priority normal\n&gt;   soap-version soap11\n&gt;   format text\n&gt;   timestamp zulu\n&gt;   fixed-format\n&gt;   local-ident \"xxx-yyy02\"\n&gt;   size 500\n&gt;   archive-mode rotate\n&gt;   upload-method ftp\n&gt;   rotate 3\n&gt;   no ansi-color\n&gt;   remote-address \"172.28.97.94\" \"1514\"\n&gt;   local-address 0.0.0.0\n&gt;   facility user\n&gt;   rate-limit 100\n&gt;   connect-timeout 60\n&gt;   idle-timeout 15\n&gt;   active-timeout 0\n&gt;   no feedback-detection\n&gt;   no event-detection\n&gt;   suppression-period 10\n&gt;   event-filter 0x080e0018d\n&gt;   event-filter 0x080e0027c\n&gt;   object \"XMLManager\"\n&gt;   ssl-client-type proxy\n&gt;   retry-interval 1\n&gt;   retry-attempts 1\n&gt;   long-retry-interval 20\n&gt;   precision microsecond\n&gt;   event \"all\" \"notice\"\n&gt; exit\n</code></pre></p> <p>Capture the output in a file, remove the lines above <code>logging target</code> and strip the leading <code>&gt;</code>. <pre><code>sed 's/^&gt; //' log-target-config.txt &gt; log-target-configmap.cfg\n</code></pre></p> </li> </ul>"},{"location":"datapower/DP-LogTarget-no-WebGUI/#define-k8socp-configmap-inject-datapower-log-target","title":"Define k8s/OCP ConfigMap &amp; Inject DataPower Log Target","text":""},{"location":"datapower/DP-LogTarget-no-WebGUI/#api-connect-datapower-installation","title":"API Connect-DataPower installation","text":"<p>This installation is on k8s.</p> <ul> <li> <p>Create ConfigMap  <pre><code>kubectl create configmap datapower-logtarget-cfg \\\n--from-file=./log-target-configmap.cfg -n dev  \n</code></pre></p> </li> <li> <p>Create <code>additionalDomainConfig</code> file for DataPower on k8s <pre><code>spec:\n  additionalDomainConfig:\n  - name: \"default\"\n    dpApp:\n      config:\n      - \"datapower-logtarget-cfg\"\n</code></pre></p> </li> <li> <p>Patch DataPower GatewayCluster with additionalDomainConfig <pre><code>kubectl patch gatewaycluster gwv6 --type merge \\\n--patch-file='DP-LogTarget-k8s-additionalDomainConfig.yaml' -n dev\n</code></pre></p> </li> <li> <p>Verify Log Target   You can examine the new <code>filebeat</code> Log Target in WebGUI or by logging into the DataPower. See DataPower Config without WebGUI for details.</p> </li> <li> <p>DataPower CLI <pre><code>co;show logging target\n</code></pre></p> </li> <li> <p>DataPower WebGUI     Should be the same as in Create Log Target using WebGUI</p> </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/","title":"DataPower Config without WebGUI","text":""},{"location":"datapower/DataPower-Config-without-WebGUI/#ibm-datapower-migrate-to-cloud","title":"IBM DataPower: Migrate to Cloud","text":"<p>Ravi Ramnarayan  \u00a9 IBM v2.7  2023-10-13  </p>"},{"location":"datapower/DataPower-Config-without-WebGUI/#datapower-config","title":"DataPower Config","text":"<p>The DataPower WebGUI makes it easy to customize domain configurations. When DataPower runs on Kubernetes (k8s), or Redhat Openshift (OCP) in conjunction with API Connect, we can create and manage configurations through k8s or OCP commands. This document posits use cases and details implementation steps for DataPower running on k8s/OCP. While Crypto objects kick started this document, other DataPower configurations can be controlled with the same approach. The document highlights the differences in implementation steps for k8s, OCP and CP4I installations.    </p>"},{"location":"datapower/DataPower-Config-without-WebGUI/#goals","title":"Goals","text":"<ul> <li>Empower IBM API Connect clients to configure DataPower k8s/OCP as they would DataPower appliances, physical or virtual  </li> <li>Provide the flow of operations and commands (CLI) to implement CI/CD  </li> <li>Preserve the evidence with files in source control  </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#target-audience","title":"Target audience","text":"<ul> <li>Experienced IT Professionals with in depth knowledge of IBM API Connect, DataPower, k8s/OCP  </li> <li>Specific items: k8s/OCP commands, DataPower commands, DataPower login k8s/OCP   </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#use-cases","title":"Use cases","text":"<p>The use cases address configurations for the <code>apiconnect</code> domain and the system wide <code>default</code> domain.  </p> <ul> <li>JWT Key in <code>apiconnect</code> domain   Almost all installations of IBM API Connect (APIC) use the JWT feature. Clients would like API to use a common JWT Key.  </li> <li>Enable <code>web-mgmt</code> in <code>default</code> domain  </li> <li>DataPower configurations in <code>default</code> and <code>apiconnect</code> domains  </li> </ul> <p>Though the use cases differ in complexity the solutions traverse the same trail.  </p> <ul> <li>Create Secrets and ConfigMaps  </li> <li>Apply <code>additionalDomainConfig</code> to the target domain      The configurations will propagate to all DataPower pods which support the <code>apiconnect</code> domain or all pods for the <code>default</code> domain. Reference: Customizing a DataPower deployment.</li> <li>Verify DataPower configurations  </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#jwt-datapower-crypto-key-in-apiconnect-domain","title":"JWT DataPower Crypto Key in <code>apiconnect</code> domain","text":"<p>Any API published to the DataPower <code>apiconnect</code> domain could use JWT key. The API may belong to different API Connect Catalogs.</p> <p>Pro: No need to republish API when the Crypto Key is changed. Contra: Creating a DataPower Crypto Key is more complex than creating an API Connect Catalog propery.  </p>"},{"location":"datapower/DataPower-Config-without-WebGUI/#secret-configmap","title":"Secret &amp; ConfigMap","text":"<ul> <li> <p>Create secret <code>kubectl create secret generic mycryptokey --from-file=./my-apic-privkey.pem -n dev</code>   The file <code>my-apic-privkey.pem</code> should contain a private key generated by <code>openssl</code>. The file contains encoded text sandwiched between <code>-----BEGIN RSA PRIVATE KEY-----</code> and <code>-----END RSA PRIVATE KEY-----</code>. In other words, it contains only the key and no meta data. Please consult documents for steps to generate certificates and keys. An example: mkjwk - JSON Web Key Generator.  </p> </li> <li> <p>Capture details of the k8s secret <code>kubectl get secret mycryptokey -n dev -o yaml</code> <pre><code>apiVersion: v1\ndata:\n  my-apic-privkey.pem: LS0tLS1CRUdJ&lt;yada yada yada&gt;U0EgUFJJVkFURSBLRVktLS0tLQo=\nkind: Secret\nmetadata:\n  creationTimestamp: \"2021-10-12T20:38:31Z\"\n  managedFields:\n  - apiVersion: v1\n    fieldsType: FieldsV1\n    fieldsV1:\n      f:data:\n        .: {}\n        f:my-apic-privkey.pem: {}\n      f:type: {}\n    manager: kubectl-create\n    operation: Update\n    time: \"2021-10-12T20:38:31Z\"\n  name: mycryptokey\n  namespace: dev\n  resourceVersion: \"2434040\"\n  selfLink: /api/v1/namespaces/dev/secrets/mycryptokey\n  uid: e4e41128-0410-4d48-9ae1-d14277d01540\ntype: Opaque\n</code></pre></p> <p>Note: The secret contains a <code>Name-Value</code> pair. - Name: <code>my-apic-privkey.pem</code> - Value: <code>LS0tLS1CRUdJ&lt;yada yada yada&gt;U0EgUFJJVkFURSBLRVktLS0tLQo=</code></p> </li> <li> <p>Create file <code>111-apiconnect-mycryptokey.cfg</code> with the content:   <pre><code>crypto\n  key \"mycryptokey\" \"cert:///my-apic-privkey.pem\"\nexit\n</code></pre></p> <p>Note: The Name from the previous step is the file which contains the crypto object.</p> </li> <li> <p>Create a k8s configmap <code>kubectl create configmap 111-apiconnect-mycryptokey-cfg --from-file=./111-apiconnect-mycryptokey.cfg -n dev</code> </p> </li> <li> <p>Ensure the configmap contains valid entries <code>kubectl get configmap 111-apiconnect-mycryptokey-cfg -n dev -o yaml</code> <pre><code>apiVersion: v1\ndata:\n  111-apiconnect-mycryptokey.cfg: |\n    crypto\n      key \"mycryptokey\" \"cert:///my-apic-privkey.pem\"\n    exit\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2021-10-29T20:15:36Z\"\n  managedFields:\n  - apiVersion: v1\n    fieldsType: FieldsV1\n    fieldsV1:\n      f:data:\n        .: {}\n        f:111-apiconnect-mycryptokey.cfg: {}\n    manager: kubectl-create\n    operation: Update\n    time: \"2021-10-29T20:15:36Z\"\n  name: 111-apiconnect-mycryptokey-cfg\n  namespace: dev\n  resourceVersion: \"3114805\"\n  selfLink: /api/v1/namespaces/dev/configmaps/111-apiconnect-mycryptokey-cfg\n  uid: e85ea025-47d5-4bb8-aaee-409a5f961f12\n</code></pre></p> </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#apply-additionaldomainconfig","title":"Apply <code>additionalDomainConfig</code>","text":"<ul> <li> <p>k8s      Apply <code>additionalDomainConfig</code> to the GatewayCluster.  </p> <ul> <li>Create file <code>251-apiconnect-k8s-additionalDomainConfig.yaml</code> with the following lines:   <pre><code># Add mycryptokey to apiconnect domain\nspec:\n  additionalDomainConfig:\n  - name: \"apiconnect\"\n    certs:\n    - certType: \"usrcerts\"\n      secret: \"mycryptokey\"\n    dpApp:\n      config:\n      - \"111-apiconnect-mycryptokey-cfg\"\n</code></pre></li> <li>Modify the DataPower Gateway Cluster in the namespace <code>dev</code> <code>kubectl patch gatewaycluster gwv6 --type merge --patch-file='251-apiconnect-k8s-additionalDomainConfig.yaml' -n dev</code> </li> </ul> </li> <li> <p>OCP      Apply <code>additionalDomainConfig</code> to the <code>gateway</code> section of the APIConnectCluster.  </p> <ul> <li> <p>Create file <code>261-apiconnect-ocp-additionalDomainConfig.yaml</code> with the following lines:   <pre><code># Add mycryptokey to apiconnect domain\nspec:\n  gateway:\n    additionalDomainConfig:\n    - name: \"apiconnect\"\n      certs:\n      - certType: \"usrcerts\"\n        secret: \"mycryptokey\"\n      dpApp:\n        config:\n        - \"111-apiconnect-mycryptokey-cfg\"\n</code></pre></p> <p>Note: <code>spec.gateway.additionalDomainConfig</code> </p> </li> <li> <p>Determine the name of the APIConnectCluster:   <pre><code># oc project dev  \n# oc get apiconnectcluster  \nNAME      READY   STATUS   VERSION        RECONCILED VERSION   AGE\napic-rr   4/4     Ready    10.0.1.5-eus   10.0.1.5-3440-eus    18h\n</code></pre></p> </li> <li> <p>Patch APIConnectCluster with <code>additionalDomainConfig</code> <code>oc patch apiconnectcluster apic-rr --type merge --patch-file='261-apiconnect-ocp-additionalDomainConfig.yaml'</code> </p> </li> <li> <p>Determine the name(s) of the DataPower pod(s)   From the above we know that <code>apic-rr</code> is the prefix for installation. <pre><code># oc get pod | grep apic-rr-gw  \napic-rr-gw-0                                                      1/1     Running     0          12m\n</code></pre>   In the lab installation there is only one DataPower pod.  </p> </li> </ul> </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#verify-datapower-crypto-object","title":"Verify DataPower crypto object","text":"<p>Wait for the gateway pod(s) to restart and attach to any gateway pod.</p> <ul> <li> <p>Log into DataPower  </p> <p>k8s: <code>kubectl attach -it gwv6-0 -c datapower -n dev</code> OCP: <code>oc attach -it apic-rr-gw-0 -c datapower -n dev</code> </p> <p><pre><code>login: admin\nPassword: *****\n\nWelcome to IBM DataPower Gateway console configuration.\nCopyright IBM Corporation 1999, 2021\n\nVersion: IDG.10.0.3.0 build 333705 on Jun 16, 2021 9:06:57 PM\nDelivery type: CD\nSerial number: 0000001\n\nidg# switch apiconnect;co\nGlobal mode\nidg[apiconnect](config)# show key mycryptokey\n\nkey: mycryptokey [up]\n----------------\nadmin-state enabled\nfile-name cert:///my-apic-privkey.pem\n\nidg[apiconnect](config)# dir cert:\n  File Name                    Last Modified                    Size\n  ---------                    -------------                    ----\n  my-apic-privkey.pem           Oct 12, 2021 8:56:03 PM          1679\n  gwd/                         Oct 12, 2021 8:56:03 PM          44\n</code></pre> <code>mycryptokey</code> uses the private key in <code>file-name cert:///my-apic-privkey.pem</code> and its status is <code>[up]</code>.  </p> </li> <li> <p>Log out from DataPower (mind the P's &amp; Q's): <code>exit;exit</code> <code>Ctrl-P Ctrl-Q</code> </p> </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#enable-web-mgmt-in-default-domain","title":"Enable <code>web-mgmt</code> in <code>default</code> domain","text":"<p>This is usually the first tweak to DataPower installations since the dawn of the WebGUI. No secrets needed. And yet, you will enable web-mgmt on all DataPower containers in the API Connect Cluster.    </p> <ul> <li> <p>Create file <code>311-default-web-mgmt.cfg</code> with the content:   <pre><code>web-mgmt\n  admin enabled\nexit\n</code></pre></p> </li> <li> <p>Create a ConfigMap <code>kubectl create configmap 311-default-web-mgmt-cfg --from-file=./311-default-web-mgmt.cfg -n dev</code> </p> </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#api-connect-on-ocp","title":"API Connect on OCP","text":"<ul> <li>File 361-default-ocp-additionalDomainConfig.yaml contains:   <pre><code>spec:\n  gateway:\n    additionalDomainConfig:\n    - name: \"default\"\n      dpApp:\n        config:\n        - \"311-default-web-mgmt-cfg\"\n</code></pre></li> <li>Determine the name of the APIConnectCluster:   <pre><code># oc project dev  \n# oc get apiconnectcluster  \nNAME      READY   STATUS   VERSION        RECONCILED VERSION   AGE\napic-rr   4/4     Ready    10.0.1.5-eus   10.0.1.5-3440-eus    18h\n</code></pre></li> <li> <p>Patch APIConnectCluster with <code>additionalDomainConfig</code> <code>oc patch apiconnectcluster apic-rr --type merge --patch-file='361-default-ocp-additionalDomainConfig.yaml'</code> </p> </li> <li> <p>Determine the name(s) of the DataPower pod(s)   From the above we know that <code>apic-rr</code> is the prefix for installation. <pre><code># oc get pod | grep apic-rr-gw  \napic-rr-gw-0                                                      1/1     Running     0          12m\n</code></pre>   In the lab installation there is only one DataPower pod.  </p> </li> <li> <p>Verify <code>web-mgmt</code> setting on DataPower    Most production installations have three DataPower pods. You could attach to any pod to verify the <code>apiconnect</code> domain configurations. <code>oc attach -it apic-rr-gw-0 -c datapower</code> </p> </li> <li> <p>Expose the WebGUI on OCP using an option below  </p> <ul> <li>Step 2 in Enable DataPower webgui in cp4i and OCP</li> <li>Command line <code>oc create route passthrough &lt;route-name-webgui&gt; --service='&lt;gwy-datapower-service-name&gt;' --hostname='&lt;webgui-name&gt;.&lt;ocp-cp4i-installation&gt;' --insecure-policy='None' --port='webgui-port'</code><ul> <li><code>&lt;route-name-webgui&gt;</code>: Should be unique within Routes, all lower case.</li> <li><code>--hostname</code>: <code>&lt;ocp-cp4i-installation&gt;</code> is the value common to all defined Routes. <code>&lt;webgui-name&gt;</code> should be unique.</li> <li><code>--insecure-policy</code>: The value <code>None</code> is case sensitive. <code>N</code> must be upper case.</li> <li><code>--port</code>: You could use <code>9090</code> or <code>webgui-port</code> which is defined in the DataPower Service.</li> </ul> </li> </ul> </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#api-connect-on-cp4i","title":"API Connect on CP4I","text":"<p>There are a few settings which can be controlled from the CP4I Openshift console. For all other DataPower configurations, you will have to use <code>additionalDomainConfig</code>.  </p> <ul> <li>API Connect on CP4I offers an option to enable WebGUI in the Openshift console. How to enable web-mgmt in cp4i? walks you through steps to enable WebGUI on CP4I. You will need to define a Route using:</li> <li>Use Step 2 in Enable DataPower webgui in cp4i and OCP to expose the WebGUI to a browser</li> <li> <p>Or use the command line (above) to expose the WebGUI  </p> </li> <li> <p>You could still use <code>additionalDomainConfig</code> on CP4I to enable WebGUI as described above for OCP.  </p> </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#changes-to-default-and-apiconnect-domains","title":"Changes to <code>default</code> and <code>apiconnect</code> domains","text":"<p>Let's combine JWT DataPower Crypto Key in <code>apiconnect</code> domain with Enable <code>web-mgmt</code> in <code>default</code> domain.  </p> <ul> <li>Define the Secrets &amp; ConfigMaps needed for both use cases  </li> <li> <p>Create 411-default-apiconnect-combo-ocp-addlDomainCfg.yaml with <pre><code># Combo default &amp; apiconnect: web-mgmt &amp; mycryptokey\nspec:\n  gateway:\n    additionalDomainConfig:\n    - name: \"default\"\n      dpApp:\n        config:\n        - \"311-default-web-mgmt-cfg\"\n    - name: \"apiconnect\"\n      certs:\n      - certType: \"usrcerts\"\n        secret: \"mycryptokey\"\n      dpApp:\n        config:\n        - \"111-apiconnect-mycryptokey-cfg\"\n</code></pre></p> </li> <li> <p>Process the combined <code>additionalDomainConfig</code> <code>oc patch apiconnectcluster apic-rr --type merge --patch-file='411-default-apiconnect-combo-ocp-addlDomainCfg.yaml'</code>   FYI... The API Connect Cluster CR will reformat the stanza  <pre><code>additionalDomainConfig:\n  - dpApp:\n      config:\n        - 311-default-web-mgmt-cfg\n    name: default\n  - certs:\n      - certType: usrcerts\n        secret: mycryptokey\n    dpApp:\n      config:\n        - 111-apiconnect-mycryptokey-cfg\n    name: apiconnect\n</code></pre></p> </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#oops","title":"Oops *!#^&amp;","text":"<p><code>additionalDomainConfig</code> is a singleton within each DataPower domain. Every time you process an <code>additionalDomainConfig</code>, you will overwrite the previous. The moving finger having writ, cleans the slate.   </p> <ul> <li> <p>You have modified a Single Domain <code>apiconnect</code> with <code>additionalDomainConfig</code>    Create an empty <code>additionalDomainConfig</code> as in sample oops file: 362-oops-apiconnect-ocp-additionalDomainConfig.yaml. Process <code>oc patch</code> APIConnectCluster CR on OCP. <pre><code>spec:\n  gateway:                    &lt;&lt;---- for APIC on OCP\n    additionalDomainConfig:\n    - name: \"apiconnect\"    \n</code></pre></p> </li> <li> <p>You have modified Two Domains <code>default</code> &amp; <code>apiconnect</code> with <code>additionalDomainConfig</code>    Let's assume you wish to remove all settings for <code>apiconnect</code> and retain the <code>web-mgmt</code> setting for the <code>default</code> domain. Create file similar to 415-oops-default-apiconnect-ocp-addlDomainCfg.yaml. Process <code>oc patch</code> APIConnectCluster CR on OCP. <pre><code># Keep default web-mgmt &amp; remove apiconnect config\nspec:\n  gateway:\n    additionalDomainConfig:\n    - name: \"default\"\n      dpApp:\n        config:\n        - \"311-default-web-mgmt-cfg\"\n    - name: \"apiconnect\"\n</code></pre>   FYI ... The API Connect Cluster CR will reformat the stanza  <pre><code>additionalDomainConfig:\n  - dpApp:\n      config:\n        - 311-default-web-mgmt-cfg\n    name: default\n  - name: apiconnect\n</code></pre></p> </li> <li> <p>If you wish to retain the existing <code>additionalDomainConfig</code> settings, weave them in with the new. Remember, the sequence of ConfigMaps in <code>dpApp.config</code> is critical.</p> </li> </ul>"},{"location":"datapower/DataPower-Config-without-WebGUI/#develop-datapower-config-on-your-desktop","title":"Develop DataPower <code>config</code> on your desktop","text":"<p>The IBM Techcon 2022 session Gateways in a container world - best practices explains steps to develop DataPower assemblies and configurations on your workstation.  </p> <p>Note: You might have to register to access the recording or download slides.  </p> <p>Correction to slide #14 <code>docker run -it -e DATAPOWER_ACCEPT_LICENSE=true \\</code> <code>-e DATAPOWER_INTERACTIVE=true \\</code> <code>-v $(pwd)/config:/opt/ibm/datapower/drouter/config \\</code> <code>-v $(pwd)/local:/opt/ibm/datapower/drouter/local \\</code> <code>-v $(pwd)/certs:/opt/ibm/datapower/root/secure/usrcerts \\</code> <code>-p 9090:9090 \\                    &lt;--- Add this line to expose WebGUI</code> <code>--name dp-dev \\</code> <code>icr.io/integration/datapower/datapower-limited:10.0.4.0</code> </p> <p>Develop complex configurations like the TLS Sever Profile using the WebGUI and extract the <code>config</code> statements from the underlying file system.</p>"},{"location":"datapower/NBLeak-playbook/","title":"NB Leak playbook","text":""},{"location":"datapower/NBLeak-playbook/#ibm-datapower-migrate-to-cloud","title":"IBM DataPower: Migrate to Cloud","text":"<p>Author: Derek Ross   </p>"},{"location":"datapower/NBLeak-playbook/#guide-for-nbleak-in-k8s","title":"Guide for NBLeak in K8's:","text":"<ul> <li>NBLeak is a monitoring tool that tracks allocations during the runtime and saves that information into a buffer.   </li> <li>It only records that information after it has been enabled and that information requires a command to dump the information from that buffer into a file.   </li> <li>That file is set to save to the temporary filesystem and that location is not modifyable (you can copy/save that file off which is usually the best approach).   </li> <li>The goal is to get an initial snapshot and then take further snapshots every 10-20% increase in memory growth.   </li> <li>The purpose of this is to get a view of the allocations growing over time as sometimes its not just a single large allocation that grows (could be many small ones).   </li> <li>The more captures and longer runtime we have inbetween; the better that data is for support to consume.   </li> <li>The only way you will know how long to wait in between capture intervals is to monitor the resources using a dashboard, CLI, or webgui status provider.   </li> <li>Last thing I will add here is that you ABSOLUTELY want to make sure you grab those files off of the temporary filesystem before you reload or before the pod terminates itself (for safe data recovery).   </li> </ul>"},{"location":"datapower/NBLeak-playbook/#recommendation","title":"Recommendation:","text":"<ul> <li>Take an initial Snapshot to give us a base capture.   </li> <li>Take a snapshot every 10-20 % growth (depending on your growth that could be a few hours, day, e.t.c)   </li> <li>Save those captures off each time to make sure we retain them in case the box increases unexpectedly.   </li> <li>Once the box approaches ~70-80% memory lets plan to have you take one last capture, stake a screenshot of the Prometheus Dashboard, and Generate an Error-Report.   </li> <li>When the data is all captures plan to reload the box when applicable so you don't top the memory out unexpectedly (causing an outage).   </li> <li>Upload all of the files.   </li> </ul>"},{"location":"datapower/NBLeak-playbook/#how-to","title":"How to:","text":"<ol> <li>Attatch to the DataPower login:  <pre><code>\"attach -it\"   \n</code></pre></li> <li> <p>Enable CLI Telnet by entering the following command into the shell you attatched to (you can change the port if needed):  <pre><code>top;co;cli telnet 127.0.0.1 2300;write mem   \n</code></pre></p> </li> <li> <p>Detach from that pod using cntrl p+q (DO NOT USE ctrl+c or you will kill the DataPower process).   </p> </li> <li> <p>Access the container by running an exec command:  <pre><code>\"exec -it podname -- bash\"   \n</code></pre></p> </li> <li> <p>Verify the Telnet service by running a bash shell command  <pre><code>\"telnet 127.0.0.1 2300\"   \n</code></pre></p> </li> <li> <p>Login to DP using your credentials (it will be noisy with running logs popping up on screen ... thats a telnet thing)      &gt; NOTE: You likely won't be able to copy paste into this shell and the password is going to be plain-text ... so fair warning.   </p> </li> <li> <p>Once you confirm that this works (run a test command in one of the domains \"show gateway-peering-status\" for example) you can then exit the bash shell using \"exit\" (may need a few exit's to get back).   </p> </li> <li> <p>At this point we are ready to run the CLI commands of interest which you can put them into a .txt file and run it through kubectl   </p> <p>Here's an example of what the file could/should look like (the ping part is just to make sure we don't exit before completing our previous command)  <pre><code>admin   \npassword   \ntop;diag;set-memory nbleak immediate;set-tracing on memory;   \nping 1.2.3.4;   \nexit;exit   \n</code></pre></p> </li> <li> <p>Once this files created and uploaded to the machine &gt; we can run that file using the following command:  <pre><code>kubectl -n &lt;namespace&gt;  exec -it &lt;pod name&gt; --  nc 127.0.0.1 2300 &lt; ourfile.txt |strings   \n</code></pre></p> <p>Some customers may need to get to kubectl first &gt; then run the exec command (command can change depending on container such as adding oc in front).  - Its also worth noting that copying directly in may fail on some containers or it may not be read properly.  - You are better off manually typing these commands as there some of the hosts for the containers can replace/mis-read characters when consuming (slashes, dashes, e.t.c)   </p> </li> <li> <p>Now the Previous step enabled the tool but in the case of say NBLeak you may still need to collect it:   </p> <ul> <li>You can do this in another file or you can login directly to run the commands yourself.   </li> <li>Here's an example of putting it into another file  <pre><code>admin   \npassword   \ntop;diag;save memory usage;   \nping 1.2.3.4;   \nexit;exit   \n</code></pre> <pre><code>kubectl -n &lt;namespace&gt;  exec -it &lt;pod name&gt; --  nc 127.0.0.1 2300 &lt; secondfile.txt |strings   \n</code></pre></li> </ul> </li> <li>The last step once you are done capturing however many sets of data you need; is to disable the tool:   <ul> <li>You again can do this by running the commands directly or creating  <pre><code>admin   \npassword   \ntop;diag;set-tracing off   \nping 1.2.3.4   \nexit;exit   \n</code></pre> <p>You may also want to kill off the pod (restart it) to make sure the NBLeak module is done   </p> </li> </ul> </li> </ol>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/","title":"Port DataPower Services to OpenShift","text":""},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#ibm-datapower-migrate-to-cloud","title":"IBM DataPower: Migrate to Cloud","text":"<p>Ravi Ramnarayan, Charlie Sumner    \u00a9 IBM v1.37  2023-10-13      </p>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#goals","title":"Goals","text":"<ul> <li>Demonstrate Proof of Technology (POT) to receive HTTP messages in DataPower and route them to the appropriate MQ queue.  </li> <li>Illustrate process to port DataPower configurations from legacy platforms to DataPower on OpenShift.</li> </ul>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#audience","title":"Audience","text":"<p>Experienced IT professionals who are experts in DataPower and OpenShift.   </p>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#acknowledgments","title":"Acknowledgments","text":"<p>Paul Faulkner contributed the MQ server, defined queues and helped setup the tool to verify message receipt.  </p>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#approach","title":"Approach","text":"<ul> <li>Legacy solution  </li> <li>An existing solution on DataPower (DPG) physical appliance, VMware or Linux     For this POT, the solution comprises MQ objects and an XSL file to route incoming messages to the appropriate queue. All configurations are in the domain MQFYRE.    </li> <li>Validate the solution with messages and MQ queues    </li> <li>Isolate configurations in <code>config</code>, <code>local</code> &amp; <code>certs</code>     This POT does not use <code>certs</code>.   </li> <li>Deploy configurations on DataPower on OpenShift (OCP)  </li> <li>Package configurations according to guidelines in Customizing a DataPower deployment and Domain Configuration </li> <li>Inject configurations into DataPower through the APIConnectCluster CR  </li> <li>Create an OCP Route to expose the DataPower MQ listener  </li> </ul>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#legacy-pot-components","title":"Legacy POT components","text":"<p>MQFYRE domain contains the files we need to migrate the POT solution to OpenShift  </p> <ul> <li>MQ related configurations are in <code>config/MQFYRE.cfg</code> </li> <li>Route logic is in <code>local/Route2qByURI.xsl</code> </li> </ul>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#diversion-from-the-datapower-expert-track","title":"Diversion from the DataPower expert track","text":"<p>Only for DataPower newbies: DataPower newbie track </p>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#deploy-domain-objects-to-datapower-on-openshift-ocp","title":"Deploy domain objects to DataPower on OpenShift (OCP)","text":"<p>The approach relies on features of <code>additionalDomainConfig</code> detailed in Customizing a DataPower deployment:   </p> <ul> <li>Modify configuration of an existing domain  </li> <li>Create a new domain, if the domain does not exist Domain Configuration defines the constructs to inject <code>config</code>, <code>certs</code>, and <code>local</code> into DataPower OCP. This POT solution does not use <code>certs</code>.  </li> </ul>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#organize-files-in-specific-directories","title":"Organize files in specific directories","text":"<p>Place <code>MQFYRE.cfg</code> in <code>config</code> and <code>Route2qByURI.xsl</code> in <code>local</code> directories:</p> <pre><code>cd &lt;path&gt;  \n$ tree\n.\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 MQFYRE.cfg\n\u2514\u2500\u2500 local\n    \u2514\u2500\u2500 Route2qByURI.xsl\n</code></pre>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#place-domain-configurations-in-configmap","title":"Place domain configurations in <code>configMap</code>","text":""},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#local","title":"<code>local</code>","text":"<p>The POT example is Route2qByURI.xsl.  </p> <ul> <li> <p>Create tarball for files in <code>&lt;path&gt;/local</code>. The DataPower operator will deploy the files to DPG <code>local:///</code> within the MQFYRE domain.   <pre><code>tar --directory=&lt;path&gt;/local -czf MQFYRE-local.tar.gz .\n</code></pre>   Ensure the tarball is correct:   <pre><code>$ tar -tzf MQFYRE-local.tar.gz\n./\n./Route2qByURI.xsl\n</code></pre>   In this POT we have only one file in <code>local</code>. If your solution calls for multiple files, place all files in <code>local</code> and create the tarball.  </p> </li> <li> <p>Create the <code>configMap</code> <pre><code>oc project &lt;namespace&gt;\noc create configmap mqfyre-local --from-file=&lt;path&gt;/MQFYRE-local.tar.gz\n</code></pre></p> </li> <li>Examine the <code>configMap</code>: <pre><code>oc get configmap mqfyre-local -o yaml  \napiVersion: v1\nbinaryData:\n  MQFYRE-local.tar.gz: H4sIAAAAAAAAA+2UW2vbMBTH85pCv4MQhaQwW3Gua6hTGKxQaAdNW+geVfskMdiyK8m57NPvyE6cpNeXljKmX0IsonP/y3JZ7dNpIYNBzzy9Qa+1+9xQ87rtXr/fH+C31vK8bqdfI73PL61Wy5XmkpCalDwRXPLX7N7b/0dx2TjNNbQff6zuxhfuUsUfn8MI3O93X9O/3fHapf7ddsvreKh/t9Pt1Ujr40t5zn+u/+nZMonJHKSKUuFTz21RAiJIw0hMfXp3e+58p2ejw4NTPBlDpVcxqBmA3vc4PKhjFKGGaOTTmdbZkLHFYuEuOm4qp8w7OTlh9zeX7FZyoSapTCgpHcJszz7kmmfpAqQbpAmDpQZhsqhthjALUjGJpm+4ZahSwkqzTZ6lil4qrI0nkd1fXd4EM0i4Ewk8DCKA3YbC9/02SdBZ71lrSLJc8sLlBjdhksdsG1ylPAMxr1xUEU25uGu2CjezYGgEcZpB4VtNxYEYEhDaySRMoiUon4YZqrcM4jwER4LK4/1NspkeRUXrhaT48me5JgnoWYqtYurn+pNIhJjHpyKlhKGr+ZTupsMYGyMJ18HMp4wW2/U6IaQwmHMZ8YcYiOAJ+DSX0cUvShSWHmhTUmXQbODKDAHkPAqA4W3UODbpCDERSclLMSNxnUMO26gqf1BaYgMOn2iQzaMi67cGezR2rAxbdrENGk2IBmW8S9cYxFTPmgIPK4+jP+CojAfQPFpnOz4mI9KiRXVFkHrx+2aheNFMQW/rRC0CrpuNMEsesfOr6/Pf45/sbAxYp9JFGr/xrcpYVr2Oj4vtkBNQik8B5R1mMkplpFc4W3jIUejCIEizlZNOqsxH61LY6JTtBBiRKrrRsE5OMaIC7Txp5IlUEg+RGVku4wYlcx5j3VUKo+C66s24WTnvqpmqqfWqNNgcrUKp9X/bC2i064TFHh589U1qsVgsFovFYrFYLBaLxWKxWCwWi8VisVgsFsvX8ReVqO8sACgAAA==\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2022-07-12T12:29:45Z\"\n  managedFields:\n  - apiVersion: v1\n    fieldsType: FieldsV1\n    fieldsV1:\n      f:binaryData:\n        .: {}\n        f:MQFYRE-local.tar.gz: {}\n    manager: kubectl-create\n    operation: Update\n    time: \"2022-07-12T12:29:45Z\"\n  name: mqfyre-local\n  namespace: cp4i\n  resourceVersion: \"122640556\"\n  uid: afd2d227-99ac-4b0a-a79f-47367a752755\n</code></pre></li> </ul>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#config","title":"<code>config</code>","text":"<p>The POT example is MQFYRE.cfg.</p> <p>Note: - The comment line (starts with \"#\") has been removed from MQFYRE.cfg. Otherwise it will scramble the <code>configMap</code>. - You should replace the following lines in <code>mq-qm</code> object with an appropriate server &amp; queue manager. <code>hostname RAVIQM1.CHARLIE.XYZ.COM(1419)</code> <code>queue-manager \"RAVIQM\"</code> </p> <pre><code>oc create configmap mqfyre-config --from-file=&lt;path&gt;/config/MQFYRE.cfg\n</code></pre> <p>Examine the <code>configMap</code>. It should have the contents of MQFYRE.cfg in plain text. If it looks scrambled, remove the comment line and try again. <pre><code>oc get configmap mqfyre-config -o yaml  \napiVersion: v1\ndata:\n  MQFYRE.cfg: |\n    top; configure terminal;\n\n    %if% available \"domain-settings\"\n\n    domain-settings\n      admin-state enabled\n      password-treatment masked\n      config-dir config:///\n    exit\n\n    %endif%\n  ...\n  ...\n  ...\n    %if% available \"wsm-agent\"\n\n    wsm-agent\n      admin-state disabled\n      max-records 3000\n      max-memory 64000\n      capture-mode faults\n      buffer-mode discard\n      no mediation-enforcement-metrics\n      max-payload-size 0\n      push-interval 100\n      push-priority normal\n    exit\n\n    %endif%\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2022-07-12T12:39:58Z\"\n  managedFields:\n  - apiVersion: v1\n    fieldsType: FieldsV1\n    fieldsV1:\n      f:data:\n        .: {}\n        f:MQFYRE.cfg: {}\n    manager: kubectl-create\n    operation: Update\n    time: \"2022-07-12T12:39:58Z\"\n  name: mqfyre-config\n  namespace: cp4i\n  resourceVersion: \"122648530\"\n  uid: 303fea14-e911-4b3c-b1ff-51dbbbc15dc8\n</code></pre></p>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#inject-domain-into-datapower","title":"Inject domain into DataPower","text":"<ul> <li> <p>Define <code>additionalDomainConfig</code> in file <code>pot-additionalDomainConfig.yaml</code>   The contents should include all previous entries and new definitions for <code>MQFYRE</code> domain. In this example, the only previous entries were to expose the WebGUI. <pre><code>spec:\n  gateway:\n    additionalDomainConfig:\n    - name: \"default\"\n      dpApp:\n        config:\n        - \"311-default-web-mgmt-cfg\"\n    - name: \"MQFYRE\"\n      dpApp:\n        config:\n        - \"mqfyre-config\"\n        local:\n        - \"mqfyre-local\"\n</code></pre></p> <p>Note: <code>spec.gateway.additionalDomainConfig</code> This POT was developed for an API Connect &amp; Datapower deployment on OpenShift.   </p> </li> <li> <p>Determine the name of the APIConnectCluster:     <pre><code>oc get apiconnectcluster  \nNAME           READY   STATUS   VERSION    RECONCILED VERSION   AGE\napis-minimum   7/7     Ready    10.0.4.0   10.0.4.0-ifix1-54    105d\n</code></pre></p> </li> <li> <p>Apply <code>additionalDomainConfig</code> <code>oc patch apiconnectcluster apis-minimum --type merge --patch-file='&lt;path&gt;/pot-additionalDomainConfig.yaml'</code> </p> </li> <li> <p>Examine the DataPower configuration  </p> <ul> <li> <p>Log into the DataPower pod with <code>admin/&lt;secret&gt;</code>: <code>oc attach -it po/apis-minimum-gw-0 -c datapower</code> </p> </li> <li> <p>Display the following via CLI or use the WebGUI   Do you see the domains <code>default</code> and <code>MQFYRE</code>? The domain <code>apiconnect</code> will appear if, as in this POT, the DataPower hosts the API Connect Gateway Service. Drill down <code>local:///</code> and locate <code>Route2qByURI.xsl</code>.   <pre><code>idg# show domains\n\nDomain     Needs save File capture Debug log Probe enabled Diagnostics Command Quiesce state Interface state Failsafe mode\n---------- ---------- ------------ --------- ------------- ----------- ------- ------------- --------------- -------------\nMQFYRE     off        off          off       off           off                               ok              none          \napiconnect off        off          off       off           off                               ok              none          \ndefault    off        off          off       off           off                               ok              none          \n\nidg# co\nGlobal mode\nidg(config)# dir local:\n  File Name                    Last Modified                    Size\n  ---------                    -------------                    ----\n  MQFYRE/                      Jul 12, 2022 1:04:04 PM          30\n  luna_config/                 Jul 12, 2022 1:04:05 PM          58\n  apiconnect/                  Jul 12, 2022 1:07:03 PM          116\n\n  176673.9 MB available to local:\n\nidg(config)# dir local:///MQFYRE\n  File Name                    Last Modified                    Size\n  ---------                    -------------                    ----\n  Route2qByURI.xsl             Jul 12, 2022 1:04:04 PM          1226\n\n  176673.2 MB available to local:///MQFYRE\n</code></pre></p> </li> <li>Logoff   <pre><code>idg(config)# exit;exit  \n</code></pre></li> <li>Detach with <code>CTRL-P-Q</code> so that the pod stays alive.     </li> </ul> </li> </ul>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#expose-the-datapower-mq-listening-port","title":"Expose the DataPower MQ listening port","text":"<p>Until now we have operated under the umbrella of the APIConnectCluster. We tweaked <code>spec.gateway</code> in the APIConnectCluster which rippled the changes to the GatewayCluster and the corresponding StatefulSet. The DataPower Service instance exposes ports such as <code>webgui-port: 9090</code>, <code>apic-gw-mgmt: 3000</code> and several others, but it is not under the control of the APIConnectCluster.</p> <p>Note: Do NOT modify the DataPower Service created during API Connect installation.  </p>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#define-a-new-service-for-the-mq-listening-port","title":"Define a new Service for the MQ listening port","text":"<ul> <li>Get DataPower <code>app.kubernetes.io/instance</code>   We need the value for the target GatewayCluster. In this case, the APIConnectCluster created the GatewayCluster instance as evidenced by the presence of the domain <code>apiconnect</code>. In other cases, the DataPower GatewayCluster might be independent from APIConnectCluster.  <pre><code>oc get pod | grep gw\napis-minim-c20517ff-mtls-gw-7bd79cf7ff-h5hbl                      1/1     Running     0          11d\napis-minimum-gw-0                                                 1/1     Running     0          3h\n\noc get pod apis-minimum-gw-0 -o yaml | grep app.kubernetes.io/instance\n  app.kubernetes.io/instance: cp4i-apis-minimum-gw\n</code></pre></li> <li> <p>Define Service mqfyre-gw-datapower.yaml   Reference: Service creation in Exposing DataPower Services. <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mqfyre-gw-datapower\nspec:\n  selector:\n    app.kubernetes.io/component: datapower\n    app.kubernetes.io/instance: cp4i-apis-minimum-gw    \n  ports:\n  - protocol: TCP\n    port: 8181\n    targetPort: 8181\n    name: mqfyre\n</code></pre></p> </li> <li> <p>Create the OpenShift Service  <code>oc apply -f mqfyre-gw-datapower.yaml</code> </p> </li> <li> <p>Verify the Service <pre><code>oc get service mqfyre-gw-datapower  \n\nNAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nmqfyre-gw-datapower   ClusterIP   172.30.66.221   &lt;none&gt;        8181/TCP   2m50s\n</code></pre></p> </li> </ul>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#create-an-openshift-route-to-the-mq-port-8181","title":"Create an OpenShift Route to the MQ port 8181","text":"<p>DataPower port 8181 is an HTTP port. Hence, mqfyre-dp2-route.yaml defines a Route without TLS. For an HTTPS port, you would modify the YAML to suit.  </p> <p>Create the Route: <code>oc apply -f &lt;path&gt;/mqfyre-dp2-route.yaml</code></p>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#validate-mq-flow","title":"Validate MQ flow","text":"<p>mq-payload.xml contains a sample payload.  </p> <p><code>curl --data-binary @mq-payload.xml MQFYRE-Route-Name/queue/{queuename}</code> where   </p> <ul> <li><code>{queuename}</code> is the name of the queue  </li> <li><code>@mq-payload.xml</code> contains the payload. <code>@</code> ensures that <code>curl</code> transmits the file as is  </li> </ul> <p>Send a message to MQ.INPUT1: <code>curl --data-binary @mq-payload.xml http://mqfyre-dp2-cp4i.ravi.charlie.rrcs.xyz.com/queue/MQ.INPUT1</code> </p> <p>And to MQ.INPUT2: <code>curl --data-binary @mq-payload.xml http://mqfyre-dp2-cp4i.ravi.charlie.rrcs.xyz.com/queue/MQ.INPUT2</code> </p>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#develop-datapower-config-on-your-desktop","title":"Develop DataPower <code>config</code> on your desktop","text":"<p>The IBM Techcon 2022 session Gateways in a container world - best practices explains steps to develop DataPower assemblies and configurations on your workstation.  </p> <p>Note: You might have to register to access the recording or download slides.  </p> <p>Correction to slide #14 <code>docker run -it -e DATAPOWER_ACCEPT_LICENSE=true \\</code> <code>-e DATAPOWER_INTERACTIVE=true \\</code> <code>-v $(pwd)/config:/opt/ibm/datapower/drouter/config \\</code> <code>-v $(pwd)/local:/opt/ibm/datapower/drouter/local \\</code> <code>-v $(pwd)/certs:/opt/ibm/datapower/root/secure/usrcerts \\</code> <code>-p 9090:9090 \\                    &lt;--- Add this line to expose WebGUI</code> <code>--name dp-dev \\</code> <code>icr.io/integration/datapower/datapower-limited:10.0.4.0</code> </p> <p>Enable WebGUI in <code>web-mgmt</code> of the <code>default</code> domain and restart the DataPower container. Develop complex configurations for this POT using the WebGUI and extract them from from the underlying file system. You could develop the POT solution on DataPower Docker using WebGUI, just as you would on other platforms.   </p>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#datapower-newbie-track","title":"DataPower newbie track","text":"<p>As a DataPower newbie (like one of the authors), you could request the DataPower guru to export configurations of DataPower objects:</p> <p>Note: The following DataPower objects are specific to this POT  </p> <ul> <li>MQ Queue Manager object - Connection to Queue Manager  - Specifies the MQ Manager Host name, Port, Queue Manager and Channel Name</li> <li>Multi-Protocol Gateway Service - Service object that accepts a transaction via HTTP, determines the backend endpoint by extracting the URI, and then performs a put to the backend queue</li> <li>HTTP Front Side Protocol Handler - Listens for HTTP traffic on Port 8181</li> <li>Custom XSL Stylesheet - Retrieves the specifed URI (/queue/{queue name}), extracts queue name with a string function <code>&lt;xsl:variable name=\"uriIN\" select=\"dp:variable('var://service/URI')\" /&gt;</code> <code>&lt;xsl:variable name=\"inQueue\" select=\"substring-after($uriIN,'/queue/')\" /&gt;</code>   and concatenates the protocol and MQ Queue Manageer name (created above) and sets the target service variable to the completed URL <code>&lt;xsl:variable name=\"target\" select=\"concat('dpmq://MQFYRE/?RequestQueue=',$inQueue)\" /&gt;</code> <code>&lt;dp:set-variable name=\"'var://service/routing-url'\" value=\"$target\" /&gt;</code> </li> </ul> <p>Payload is sent to DataPower Gateway using the <code>hostname:port/queue/{queuename}</code>. If URI is not specified, the payload will be sent to default queue (specified in MPGW service configuration).  If <code>{queuename}</code> is incorrect or invalid, the transaction will be canceled as MQ Open will fail.  </p> <ul> <li>DP-2-MQ-export.zip contains the POT configuration  </li> </ul>"},{"location":"datapower/Port-DataPower-Service-to-OpenShift/#datapower-docker","title":"DataPower Docker","text":"<ul> <li>Create the domain MQFYRE (defaults for all settings)  </li> <li>Import configurations in DP-2-MQ-export.zip </li> <li>Examine the logic on WebGUI  </li> <li> <p>Capture DataPower configurations in <code>certs</code>, <code>config</code> &amp; <code>local</code>. <pre><code>$ tree dp-dev/\ndp-dev/\n\u251c\u2500\u2500 certs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 luna_cert\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 MQFYRE\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 webgui-privkey.pem\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 webgui-sscert.pem\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 auto-startup.cfg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 auto-user.cfg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 default.cfg\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 MQFYRE\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 MQFYRE.cfg\n\u2514\u2500\u2500 local\n    \u251c\u2500\u2500 luna_config\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 Chrystoki.conf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 configData\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 token\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 001\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 data\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 client_identities\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 partition_identities\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 partition_policy_templates\n    \u2514\u2500\u2500 MQFYRE\n        \u2514\u2500\u2500 Route2qByURI.xsl\n\n15 directories, 8 files\n</code></pre></p> </li> <li> <p>MQ related configurations are in <code>config/MQFYRE/MQFYRE.cfg</code> </p> </li> <li>Route logic is in <code>local/MQFYRE/Route2qByURI.xsl</code> </li> </ul> <p>Return to Diversion from the DataPower expert track.</p>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/","title":"API Products &amp; Consumer Subscriptions","text":""},{"location":"dev-ops/Publish-APIVersion-to-COrg/#ibm-api-connect-devops-at-scale","title":"IBM API Connect: DevOps at Scale","text":"<p>Ravi Ramnarayan  \u00a9 IBM v1.62  2021-12-20    </p>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/#goals","title":"Goals","text":"<ul> <li>Ensure business continuity while publishing new versions of API Products  </li> <li>Control consumer subscriptions to API Products  </li> <li>Allow API provider teams to develop and test API quickly  </li> <li>Illustrate commands to use in DevOps</li> </ul>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/#collateral-effects","title":"Collateral effects","text":"<ul> <li>Deprecate and Retire API Products gracefully  </li> </ul>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/#prologue","title":"Prologue","text":"<p>This article provides sample code to implement DevOps processes described in Governance Models &amp; Version Control for API Products.</p>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/#pub-sub-flow","title":"Pub Sub Flow","text":"<p>The goal is to implement devops processes at scale.  </p>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/#simple-scenario","title":"Simple Scenario","text":"<ul> <li>API api:v1 is in Product product:v1 and available to consumers through plan-A &amp; plan-B </li> <li>Consumer applications, con-app-1 &amp; con-app-2 subscribe to plan-A </li> <li>Migrate Subscriptions which belong to a single consumer organization con-app-2 to product:v2 and plan-B    Consumer organizations could subscribe an application to one or more versions of same product. In addition, they could subscribe to other API products with the same application. As consumer organizations can create many applications, we could have thousands of subscriptions to a product:version.</li> </ul>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/#commands-in-apic-toolkit","title":"Commands in <code>apic toolkit</code>","text":"<p>When you publish a new version of a product, Managing API Products offers three commands which migrate all subscriptions from the source to the target product.  </p> <ul> <li><code>products:replace</code> retires the old product and transfers subscriptions to the new.   </li> <li><code>products:supersede</code>: deprecates the old product and transfers subscriptions to the new. You can retire the old product at a later date.      </li> <li><code>products:set-migration-target</code>: allows the Provider to guide Consumer App Developers to migrate subscriptions as specified in <code>PRODUCT_PLAN_MAPPING_FILE</code>.   </li> <li><code>products:execute-migration-target</code>: allows the Provider to migrate subscriptions which were processed by <code>products:set-migration-target</code>. Providers could preempt migration by App Developers or act on behalf of laggards.   </li> </ul> <p>Another command is available, though it is not shown in Managing API Products as of March 2021.  </p> <ul> <li><code>products:migrate-subscriptions</code>: allows the Provider to migrate subscriptions specified in <code>MIGRATE_SUBSCRIPTION_SUBSET_FILE</code>.</li> </ul>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/#use-case-migrate-selected-consumer-organizations-to-the-new-product","title":"Use Case: Migrate selected consumer organizations to the new product","text":"<p>Possible drivers are:  </p> <ul> <li>The first few customers might be beta testers.</li> <li>For products with a large number of subscriptions, you could to reduce the impact of change on two fronts:  </li> <li>Control the load on API Connect's publish operation   </li> <li>Handle requests for assistance from a small group of consumers   </li> </ul>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/#sample-scripts","title":"Sample scripts","text":"<p>The following steps implement the Simple Scenario and migrate subscriptions using the command <code>products:migrate-subscriptions</code>. Sample shell scripts illustrate the use of <code>apic toolkit</code> commands. You should modify and enhance them for use in your API Connect installations. For example, you could operate on more than one Consumer Organization in a single run or modify the commands to run within Catalog/Space.   </p> <ul> <li><code>product-subscriptions-for-corg.sh</code>   Provides a CSV file with <code>Consumer-Org,App-Name,Source-Product:Version,Plan-Name</code> for a given Consumer Organization and Product:Version. You can assess the inventory of subscriptions and plan operations.  </li> <li><code>products-migrate-subscriptions-1.sh</code> Creates the <code>MIGRATE_SUBSCRIPTION_SUBSET_FILE</code> for a single Consumer Organization. Source &amp; Target Products are assumed to have one for one, identically named plans. You could modify the file to alter the mapping. To effect the Simple Scenario, you would modify the <code>MIGRATE_SUBSCRIPTION_SUBSET_FILE</code>:</li> <li>From: product:v1 / plan-A ==&gt; product:v2 / plan-A </li> <li>To: product:v1 / plan-A ==&gt; to product:v2 / plan-B </li> </ul> <p>Other variations are possible. For example, you could combine subscriptions from two source plans into one target plan:</p> <ul> <li>product:v1 / plan-A ==&gt; product:v2 / plan-B </li> <li>product:v1 / plan-B ==&gt; product:v2 / plan-B </li> </ul> <p>The mapping lines above are logical, easy to read statements. For the exact syntax, please see notes in <code>products-migrate-subscriptions-1.sh</code>. You must have one entry for each Source Product Plan and the Target Plans should valid.  </p> <ul> <li><code>products-migrate-subscriptions-2.sh</code> Runs the command <code>products:migrate-subscriptions</code> on  <code>MIGRATE_SUBSCRIPTION_SUBSET_FILE</code>. The command does not change the lifecycle state of the Product. If the Product state was published, it will stay the same after the script completes.  </li> </ul> <p>Recommendation: Deprecate the Source Product:Version to prevent new subscriptions. Depending on your corporate policies, you could deprecate the Source Product:Version in <code>products-migrate-subscriptions-1.sh</code> or in <code>products-migrate-subscriptions-2.sh</code>.</p>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/#prerequisites","title":"Prerequisites","text":"<ul> <li>IBM API Connect experience, Linux commands</li> <li><code>apic toolkit</code> &amp; <code>jq</code></li> <li><code>apic login</code> to the management service with role which can publish Products  </li> <li>Scripts were developed and verified on IBM API Connect v10.0.1.x</li> </ul>"},{"location":"dev-ops/Publish-APIVersion-to-COrg/#usage","title":"Usage","text":"<p>This document and the sample scripts show what is possible with Product Versions &amp; Consumer Subscriptions. Does it fit into your overall process?</p>"},{"location":"dev-ops/api-key-for-sso/","title":"Generate API Key for SSO Login in 7 Steps","text":""},{"location":"dev-ops/api-key-for-sso/#ibm-api-connect-devops-at-scale","title":"IBM API Connect: DevOps at Scale","text":"<p>Sila Kissuu  \u00a9 IBM v1.0  2023-01-18  </p>"},{"location":"dev-ops/api-key-for-sso/#problem","title":"Problem:","text":"<p>It is common for APIC deployments to leverage existing enterprise Continuous Integration/Continuous Deployment (CI/CD) practices to automate code delivery. Typically, a pipeline is configured to log onto API Manager and execute a series of CLI commands.</p> <p>However, when authentication is performed via an external provider, such as OIDC with Single Sing-On (SSO), the default CLI behavior requires the user to authenticate via a browser in order to retrieve the required API key. </p> <p>Here is an example that uses Azure AD as the OIDC provider:</p> <p><pre><code>apic login -s apim.lab.company.com \\\n-u pipelines@company.com \\ \n-r provider/azure-oidc --context provider \\\n--sso login.microsoftonline.com\n\nPlease copy and paste the url https://apim.lab.company.com/manager/auth/manager/sign-in/?from=TOOLKIT \nto a browser to start the authentication process.\nDo you want to open the url in default browser? [y/n]: y\nAPI Key?\n</code></pre> For purposes of CICD, this is not practical to automate due to the required human interaction. </p> <p>We provide a solution.</p>"},{"location":"dev-ops/api-key-for-sso/#solution","title":"Solution:","text":"<p>Use the CLI command <code>apic api-key:create</code> to create a custom key with a custom TTL and other properties.</p>"},{"location":"dev-ops/api-key-for-sso/#requirements","title":"Requirements:","text":"<ul> <li>An input file describing the API key metadata</li> <li>User URL: this property is retrieved separately</li> </ul>"},{"location":"dev-ops/api-key-for-sso/#procedure","title":"Procedure","text":"<ol> <li> <p>Obtain the \u201cuser URL\u201d for the userID that will be associated with this API key</p> <ul> <li>Logon to APIM (provider realm) as using your OIDC user ID <pre><code>apic login -s apim.lab.company.com \\\n-u pipelines@company.com \\ \n-r provider/azure-oidc --context provider \\\n--sso login.microsoftonline.com\n\nPlease copy and paste the url https://apim.lab.company.com/manager/auth/manager/sign-in/?from=TOOLKIT \nto a browser to start the authentication process.\nDo you want to open the url in default browser? [y/n]: y\nAPI Key? &lt;&lt;paste your API key copied from the browser&gt;&gt;\n</code></pre> NOTE: this is the only time you will have to interact with the browser.</li> </ul> </li> <li> <p>List org members using <code>apic members:list</code> <pre><code>apic members:list -s apim.lab.company.com --scope catalog -c sandbox -o my-org\n\npipelines-company.com-from-azure-oidc-of-type-standard    [state: enabled]   https://platform.lab.company.com/api/catalogs/84bce7eb-354e-49c9-8d6b-48e82ac45084/e0843be8-0beb-43d5-b304-e55861e45f2c/members/4c0764a9-a4d8-4926-9759-1d3d02334c52\n</code></pre></p> <p>The output shows the definition for user <code>pipelines@company.com</code> enabled in  a Standard type OIDC configuration named <code>azure-oidc</code>.</p> <p>Copy the URL from the output - you will need it in the next step.</p> </li> <li> <p>Create a JSON file (we will name it my-key-definition.json) to define metadata for your new API key. Paste in the URL from Step 2 into the <code>`user_url</code> property. Here is a sample API key creation file:     <pre><code>{\n    \"type\": \"api_key\",\n    \"api_version\": \"2.0.0\",\n    \"name\": \"PipelineKey\",\n    \"title\": \"PipelineKey\",\n    \"summary\": \"Use this key for SSO login.\",\n    \"client_type\": \"toolkit\",\n    \"realm\": \"provider\",\n    \"user_url\": \"https://platform.lab.company.com/api/catalogs/84bce7eb-354e-49c9-8d6b-48e82ac45084/e0843be8-0beb-43d5-b304-e55861e45f2c/members/4c0764a9-a4d8-4926-9759-1d3d02334c52\",\n    \"description\": \"API Key for CI/CD operations\",\n    \"ttl\": 62294394\n}\n</code></pre></p> <p>NOTE: The value for <code>user_url</code> is derived from Step 2. </p> </li> <li> <p>Create the API key using the <code>api-keys:create</code> command, passing the file created in Step 3.     <pre><code>apic api-keys:create -s apim.lab.company.com my-key-definition.json\n\nPipelineKey   https://platform.lab.company.com/api/cloud/api-keys/b6882e4a-4b8e-43b0-963f-0478b79c7948\n</code></pre></p> </li> <li> <p>Verify the API key was created  </p> <ul> <li> <p>List API keys <pre><code>apic api-keys:list -s apim.lab.company.com\n\nPipelineKey   https://platform.lab.company.com/api/cloud/api-keys/b6882e4a-4b8e-43b0-963f-0478b79c7948\n</code></pre></p> </li> <li> <p>Retrieve the API key you just created: <pre><code>apic api-keys get KasovoniKey-3 -s apim.lab.company.com --format json\n\nPipelineKey   PipelineKey.json   https://platform.lab.company.com/api/cloud/api-keys/b6882e4a-4b8e-43b0-963f-0478b79c7948\n</code></pre> A json file named after the name of your new API key (in this example, PipelineKey), is dumped into you current directory.</p> </li> </ul> <p>THis file represents your API key definition in API Manager.</p> </li> <li> <p>Review the contents of the json file:     <pre><code>{\n    \"type\": \"api_key\",\n    \"api_version\": \"2.0.0\",\n    \"id\": \"3c0225f7-0aad-45ce-a199-ced575befb9c\",\n    \"name\": \"PipelineKey\",\n    \"title\": \"PipelineKey\",\n    \"summary\": \"Use this key for SSO login.\",\n    \"client_type\": \"toolkit\",\n    \"realm\": \"provider/azure-oidc\",\n    \"user_url\": \"https://platform.lab.company.com/api/catalogs/84bce7eb-354e-49c9-8d6b-48e82ac45084/e0843be8-0beb-43d5-b304-e55861e45f2c/members/4c0764a9-a4d8-4926-9759-1d3d02334c52\",\n    \"id_token\": \"eyJhbGciOiJSUzI1Ni...bT4IX7w\",\n    \"token_exp\": 1674104837,\n    \"token_iat\": 1674076037,\n    \"token_jti\": \"c73472c2-8c93-4fb6-bc8e-9b6a88775dfd\",\n    **_\"api_key\"_**: \"14f27ecd-919a-41f5-a4f1-2a6012cb471c\",\n    \"description\": \"API Key for CI/CD operations\",\n    \"created_at\": \"2023-01-18T21:35:19.000Z\",\n    \"updated_at\": \"2023-01-18T21:35:19.000Z\",\n    \"url\": \"https://platform.lab.company.com/api/cloud/api-keys/b6882e4a-4b8e-43b0-963f-0478b79c7948\"\n}\n</code></pre></p> <p>Your new API key, ready for use, is highlighted above. <pre><code>\"api_key\": \"14f27ecd-919a-41f5-a4f1-2a6012cb471c\"\n</code></pre></p> </li> <li> <p>Test the new API key     <pre><code>apic login -s apim.lab.company.com \\\n-u pipelines@company.com \\\n-r provider/azure-oidc \\\n--context provider \\\n--sso login.microsoftonline.com \\\n--apiKey 14f27ecd-919a-41f5-a4f1-2a6012cb471c\n\nLogged into apim.lab.company.com successfully\n</code></pre></p> </li> </ol> <p>We have successfully logged into APIM and can now proceed with CI/CD operations.</p>"},{"location":"dev-ops/api-key-for-sso/#conclusion","title":"Conclusion","text":"<p>You are now able to automate CI/CD operations using a pre-defined API key.</p>"},{"location":"dev-ops/api-key-for-sso/#references","title":"References","text":"<ul> <li>CLI Tool Reference</li> <li>Configuring an OIDC user registry</li> <li>API Connect Platform REST APIs</li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/","title":"Bastion for IBM Cloud Pak on Airgap OpenShift","text":"<p>Ravi Ramnarayan  \u00a9 IBM v3.89  2023-05-22     </p>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#goals","title":"Goals","text":"<p>For IBM clients who operate disconnected (airgap) IT, the bastion host and registry are long term investments. This document provides steps to build and maintain the bastion host and container registry for IBM Cloud Pak for Integration (CP4I) products.  </p> <ul> <li>Build insecure or secure image registries  </li> <li>Update the registry  </li> <li>Rebuild the registry   </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#target-audience","title":"Target Audience","text":"<ul> <li>IT Professionals with in depth knowledge of Linux, Kubernetes (k8s) and Openshift (OCP)    The skills to build the bastion host and the private container registry.  </li> <li>Experience with IBM Cloud Paks, API Connect and DataPower     If you wish to install IBM API Connect (APIC) after building the bastion and registry.  </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#configure-linux-bastion","title":"Configure Linux Bastion","text":"<p>Our focus is Mirroring images with a bastion host. For the larger context, please see Mirroring images for an air-gapped cluster.   </p>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#bastion-os-tools","title":"Bastion OS &amp; tools","text":"<ul> <li>RHEL 8.6, 9.x work okay    </li> <li><code>root</code> user   For expedience, it is convenient to run as <code>root</code>. Not good practice.  </li> <li>Verify <code>openssl version &gt;= 1.1.1</code> </li> <li>Install <code>podman</code> &amp; <code>httpd-tools</code> <code>dnf install podman</code> <code>dnf install httpd-tools</code> </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#install-ocpcp4i-bastion-tools","title":"Install OCP/CP4I bastion tools","text":"<ul> <li> <p>OpenShift <code>oc</code> command   Get it from your OCP installation. <pre><code>mv oc /usr/local/bin/\n$ oc version\nClient Version: 4.8.36\n</code></pre></p> </li> <li> <p>Install <code>ibm-pak</code>   Download the latest version from IBM/ibm-pak/releases.   <pre><code>tar -xf oc-ibm_pak-linux-amd64.tar.gz  \nmv oc-ibm_pak-linux-amd64 /usr/local/bin/oc-ibm_pak  \n</code></pre>   Confirm <code>ibm-pak</code> installation: <code>oc ibm-pak --help</code> </p> </li> <li> <p>Set color ouptut (optional) <code>oc ibm-pak config color --enable true</code> </p> </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#env-variables-for-apic-images","title":"ENV Variables for APIC images","text":"<ul> <li> <p>Environment variables  Mirroring images with a bastion host section Set environment variables and download CASE files specifies <code>env</code> settings. Operator, operand, and CASE versions contains appropriate values for the desired APIC version. <pre><code># For APIC  \nexport CASE_NAME=ibm-apiconnect\nexport CASE_VERSION=4.0.2\nexport ARCH=amd64\n\nexport TARGET_REGISTRY=`hostname -f`:5000\n\n# File to hold cp.icr.io authentication\n# *** Do NOT set this env at the start ***\n# Follow the steps in this document  \n# export REGISTRY_AUTH_FILE=/opt/registry/auth/cp-auth.json\n</code></pre></p> </li> <li> <p>Activate &amp; verify environment variables    <pre><code>  $ echo $CASE_VERSION\n  4.0.2\n</code></pre></p> </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#private-container-registry","title":"Private container registry","text":"<p>Your enterprise might have an existing private container registry. You could:  </p> <ul> <li>Use the existing corporate registry for IBM Cloud Pak images  </li> <li>Create a new registry for IBM Cloud Pak images   You can make the private container registry insecure or secure, depending on your corporate standards. The insecure registry is the main flow of the document. Two tables contrast simple steps for Insecure &amp; Secure registries. Complex Secure registry steps are in call out segments.     </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#create-local-repositories-for-authentication-certificates-data","title":"Create local repositories for authentication, certificates &amp; data","text":"<p>Note: instead of <code>/opt</code>, choose a base directory which is under your (non-root user) control.       </p> <pre><code>mkdir -p /opt/registry/{auth,certs,data}\n</code></pre> <ul> <li>Generate basic auth credentials <pre><code>podman run --entrypoint htpasswd \\\n  docker.io/library/httpd:2 -Bbn &lt;user&gt; &lt;paswd&gt; &gt; /opt/registry/auth/htpasswd\n</code></pre> <ul> <li>b provides the password via command  </li> <li>B stores the password using Bcrypt encryption  </li> <li>n display in standard output    </li> </ul> </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#create-an-insecure-container-registry","title":"Create an insecure container registry","text":"<pre><code>podman run -d --name apic-registry -p 5000:5000 \\\n  -v /opt/registry/data:/var/lib/registry \\\n  -v /opt/registry/auth:/auth \\\n  -e \"REGISTRY_AUTH=htpasswd\" \\\n  -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\\n  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\\n  --restart=always \\\n  docker.io/library/registry:2\n</code></pre>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#call-out-1-create-a-secure-container-registry","title":"Call Out 1: Create a secure container registry","text":""},{"location":"dev-ops/bastion-airgap-ibm-pak/#return-from-call-out-1","title":"Return from Call Out 1","text":""},{"location":"dev-ops/bastion-airgap-ibm-pak/#mirror-images-to-your-private-container-registry","title":"Mirror images to your private container registry","text":""},{"location":"dev-ops/bastion-airgap-ibm-pak/#setup-ibm-pak","title":"Setup <code>ibm-pak</code>","text":"<ul> <li> <p>Configure the plug-in to download CASEs as OCI artifacts from IBM Cloud Container Registry (ICCR). <pre><code>oc ibm-pak config repo 'IBM Cloud-Pak OCI registry' \\\n  -r oci:cp.icr.io/cpopen --enable  \n</code></pre></p> </li> <li> <p>Download the API Connect image inventory   <pre><code>oc ibm-pak get $CASE_NAME \\\n  --version $CASE_VERSION\n</code></pre>   The files will be placed in <code>$HOME/.ibm-pak</code>.  </p> </li> <li> <p>Generate mirror manifests <pre><code>oc ibm-pak generate mirror-manifests \\\n  $CASE_NAME $TARGET_REGISTRY --version $CASE_VERSION  \n</code></pre>   The command creates files to configure OCP in <code>~/.ibm-pak/data/mirror/$CASE_NAME/$CASE_VERSION</code>: <code>catalog-sources.yaml</code> <code>catalog-sources-linux-&lt;arch&gt;.yaml</code> (arch specific catalog sources) <code>image-content-source-policy.yaml</code> <code>images-mapping.txt</code> </p> </li> <li> <p>Preemptive fix for CASE_NAME <code>ibm-apiconnect</code>   Apply the fix described in Airgap install failure due to <code>unable to retrieve source image docker.io</code> to <code>~/.ibm-pak/data/mirror/$CASE_NAME/$CASE_VERSION/images-mapping.txt</code>. Please use the workaround as long as the link Airgap install failure due to 'unable to retrieve source image docker.io' is active.  </p> <p>Note: Include this step in your dev-ops process.</p> </li> <li> <p>Configure credentials for the IBM Entitled Registry on Bastion</p> <ul> <li>Define <code>$REGISTRY_AUTH_FILE</code> file to hold credentials  </li> </ul> <pre><code># File to hold cp.icr.io authentication for APIC\nexport REGISTRY_AUTH_FILE=/opt/registry/auth/cp-auth.json\n</code></pre> <ul> <li>Populate <code>$REGISTRY_AUTH_FILE</code> with IBM Entitlement Key credentials for <code>cp.icr.io</code> </li> </ul> <pre><code># podman login cp.icr.io\nUsername: cp\nPassword: &lt;IBM Entitlement Key&gt;\nLogin Succeeded!\n</code></pre> <ul> <li>Populate <code>$REGISTRY_AUTH_FILE</code> with <code>apic-registry</code> credentials  </li> </ul> Insecure Registry Secure Registry <code># podman login $TARGET_REGISTRY --tls-verify=false</code> <code># podman login $TARGET_REGISTRY</code> <code>Username: &lt;user&gt;</code> <code>Username: &lt;user&gt;</code> <code>Password: &lt;password&gt;</code> <code>Password: &lt;password&gt;</code> <code>Login Succeeded!</code> <code>Login Succeeded!</code> <ul> <li>Confirm $REGISTRY_AUTH_FILE contains both credentials <pre><code>cat $REGISTRY_AUTH_FILE\n</code></pre></li> </ul> </li> </ul> <p>Note: Backup specific files and folders from <code>~/.ibm-pak</code>, as detailed in Rebuild bastion repository.  </p>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#populate-bastion-with-images","title":"Populate bastion with images","text":"<p>Fork the job to the background, as it could run for hours.</p>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#redable-version-of-oc-image-mirror-command","title":"Redable version of <code>oc image mirror</code> command","text":"Insecure Registry Secure Registry <code>nohup oc image mirror \\</code> <code>nohup oc image mirror \\</code> <code>-f ~/.ibm-pak/data/mirror/$CASE_NAME/$CASE_VERSION/images-mapping.txt \\</code> <code>-f ~/.ibm-pak/data/mirror/$CASE_NAME/$CASE_VERSION/images-mapping.txt \\</code> <code>-a $REGISTRY_AUTH_FILE \\</code> <code>-a $REGISTRY_AUTH_FILE \\</code> <code>--insecure  \\</code> <code>--filter-by-os '.*'  \\</code> <code>--filter-by-os '.*'  \\</code> <code>--skip-multiple-scopes \\</code> <code>--skip-multiple-scopes \\</code> <code>--max-per-registry=1 \\</code> <code>--max-per-registry=1 \\</code> <code>--continue-on-error=true &gt; my-mirror-progress.txt  2&gt;&amp;1 &amp;</code> <code>--continue-on-error=true &gt; my-mirror-progress.txt  2&gt;&amp;1 &amp;</code>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#copypaste-friendly-version-of-oc-image-mirror-command","title":"Copy/paste friendly version of <code>oc image mirror</code> command","text":"Insecure Registry Secure Registry <code>nohup oc image mirror -f ~/.ibm-pak/data/mirror/$CASE_NAME/$CASE_VERSION/images-mapping.txt -a $REGISTRY_AUTH_FILE --insecure --filter-by-os '.*' --skip-multiple-scopes --max-per-registry=1 --continue-on-error=true &gt; my-mirror-progress.txt  2&gt;&amp;1 &amp;</code> <code>nohup oc image mirror -f ~/.ibm-pak/data/mirror/$CASE_NAME/$CASE_VERSION/images-mapping.txt -a $REGISTRY_AUTH_FILE --filter-by-os '.*' --skip-multiple-scopes --max-per-registry=1 --continue-on-error=true &gt; my-mirror-progress.txt  2&gt;&amp;1 &amp;</code>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#rinse-repeat","title":"Rinse &amp; repeat","text":"<p>Observe progress with <code>ps &lt;pid&gt;</code>, <code>top</code> and <code>tail my-mirror-progress.txt</code>. When the job completes, search <code>my-mirror-progress.txt</code> for \"<code>error</code>\".   </p> <ul> <li>If the job fails with \"<code>too many requests</code>\", run it again.    Rinse &amp; repeat until it completes with \"<code>info: Mirroring completed in 8m33.3s (12.65MB/s)</code>\".    </li> <li>If you see \"<code>unable to retrieve source image docker.io/ibmcom/</code>\":   Apply the fix described in Airgap install failure due to 'unable to retrieve source image docker.io' to <code>~/.ibm-pak/data/mirror/$CASE_NAME/$CASE_VERSION/images-mapping.txt</code>. And run the <code>oc image mirror</code> command again.  </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#configure-the-ocp-cluster","title":"Configure the OCP cluster","text":"<p>Login to OCP CLI   </p> <pre><code>oc login --token=sha256~ejwjQ79...nvFbQYYyOc --server=https://api.rr4.cp.myco.com:6443\nLogged into \"https://api.rr4.cp.myco.com:6443\" as \"kube:admin\" using the token provided.\n\nYou have access to 63 projects, the list has been suppressed. You can list all projects with 'oc projects'  \n\nUsing project \"default\".  \n</code></pre>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#configure-the-ocp-pull-secret-for-the-apic-registry","title":"Configure the OCP pull-secret for the APIC registry","text":"<p>Steps are from Updating the global cluster pull secret</p> <p>You may use the steps below or use OCP GUI.  </p> <ul> <li>Download the OCP <code>pull-secret</code> <pre><code>export PULL_SECRET_LOCATION=\"pull_secret_location.json\"  \noc get secret/pull-secret -n openshift-config \\\n  --template='{{index .data \".dockerconfigjson\" | base64decode}}' &gt; \\\n  $PULL_SECRET_LOCATION  \n</code></pre></li> <li> <p>Append your registry credentials to $PULL_SECRET_LOCATION    The command will NOT change existing credentials.   <pre><code>oc registry login --registry=\"$TARGET_REGISTRY\" \\\n  --auth-basic=\"&lt;user&gt;:&lt;paswd&gt;\" --to=$PULL_SECRET_LOCATION   \n</code></pre></p> </li> <li> <p>Update the OCP <code>pull-secret</code> </p> <p>Note: The command will replace the pull-secret with the file's contents.  </p> <pre><code>oc set data secret/pull-secret -n openshift-config \\\n  --from-file=.dockerconfigjson=$PULL_SECRET_LOCATION\n</code></pre> </li> <li> <p>Verify the OCP <code>pull-secret</code> using the OCP GUI  </p> </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#create-the-ocp-imagecontentsourcepolicy-icsp","title":"Create the OCP ImageContentSourcePolicy (ICSP)","text":"<ul> <li>Is ImageContentSourcePolicy defined for $CASE_NAME in OCP?   <pre><code>oc get ImageContentSourcePolicy $CASE_NAME -o yaml  \n</code></pre></li> <li>Examine the file generated in Mirror images to your private container registry <pre><code>view ~/.ibm-pak/data/mirror/$CASE_NAME/$CASE_VERSION/image-content-source-policy.yaml  \n</code></pre>   The settings should be appropriate, unless you have special reasons to modify ImageContentSourcePolicy for <code>ibm-apiconnct</code>. <pre><code>oc apply -f  \\\n  ~/.ibm-pak/data/mirror/$CASE_NAME/$CASE_VERSION/image-content-source-policy.yaml  \n</code></pre></li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#connect-your-registry-to-ocp","title":"Connect your registry to OCP","text":""},{"location":"dev-ops/bastion-airgap-ibm-pak/#insecure-registry","title":"Insecure Registry","text":"<p>Recommend defining insecure registries manually in OCP. Edit the <code>image.config.openshift.io/cluster</code> and insert the bastion docker registry. You might find more than one insecure docker registries in use. For example, the client might use a different registry for OCP images. Slide the current registry into the mix.  </p> <ul> <li> <p>Examine existing insecure registries in OCP  <pre><code>oc get image.config.openshift.io/cluster -o yaml | grep -i -B2 -A4 insecureRegistries  \nspec:\n  registrySources:\n    insecureRegistries:\n    - already-here.myco.com:5000\nstatus:\n  internalRegistryHostname: image-registry.openshift-image-registry.svc:5000  \n</code></pre>   In this case, there is an insecure registry. Edit the YAML manually on the OCP GUI, the interactive CLI <code>oc edit</code> or by downloading the YAML file. Alternatively, you could use <code>oc patch</code> with carefully composed JSON which specifies current and additonal registries. </p> </li> <li> <p>The following command will result in a single insecure registry, even if there were others in the list. For lab use only:   <pre><code>oc patch image.config.openshift.io/cluster --type=merge \\\n  -p '{\"spec\":{\"registrySources\":{\"insecureRegistries\":[\"'$TARGET_REGISTRY'\"]}}}'  \n</code></pre></p> </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#call-out-2-secure-registry","title":"Call Out 2 Secure Registry","text":""},{"location":"dev-ops/bastion-airgap-ibm-pak/#return-from-call-out-2","title":"Return from Call Out 2","text":""},{"location":"dev-ops/bastion-airgap-ibm-pak/#install-catalog-sources","title":"Install Catalog Sources","text":"<p>Note: Ensure all Nodes &amp; MachineConfigPools are Ready.  </p> <p>Adding catalog sources to a cluster, section Adding specific catalog sources for each operator:  </p> <ul> <li> <p>Apply catalog sources <pre><code># oc apply -f ~/.ibm-pak/data/mirror/${CASE_NAME}/${CASE_VERSION}/catalog-sources.yaml\ncatalogsource.operators.coreos.com/cloud-native-postgresql-catalog created\ncatalogsource.operators.coreos.com/ibm-apiconnect-catalog created\ncatalogsource.operators.coreos.com/opencloud-operators created\n</code></pre></p> </li> <li> <p>Apply \\$ARCH specific catalog sources, if such files exist:     <pre><code># oc apply -f \\\n  ~/.ibm-pak/data/mirror/${CASE_NAME}/${CASE_VERSION}/catalog-sources-linux-${ARCH}.yaml\ncatalogsource.operators.coreos.com/ibm-datapower-operator-catalog created\n</code></pre></p> </li> <li>List catalog sources defined in OCP: <pre><code># oc get catalogsource -n openshift-marketplace\nNAME                              DISPLAY                                             TYPE   PUBLISHER   AGE\ncertified-operators               Certified Operators                                 grpc   Red Hat     5h39m\ncloud-native-postgresql-catalog   ibm-cloud-native-postgresql-4.8.0+20221102.113620   grpc   IBM         2m48s\ncommunity-operators               Community Operators                                 grpc   Red Hat     5h39m\nibm-apiconnect-catalog            ibm-apiconnect-4.0.2                                grpc   IBM         2m48s\nibm-datapower-operator-catalog    ibm-datapower-operator-1.6.5-linux-amd64            grpc   IBM         70s\nopencloud-operators               ibm-cp-common-services-1.17.0                       grpc   IBM         2m48s\nredhat-marketplace                Red Hat Marketplace                                 grpc   Red Hat     5h39m\nredhat-operators                  Red Hat Operators                                   grpc   Red Hat     5h39m\n</code></pre></li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#install-operators-api-connect","title":"Install Operators &amp; API Connect","text":"<p>You can proceed to create a namespace, storage classes, install the IBM API Connect operator in the desired namespace and create an instance of APIC.</p>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#update-apic-with-new-release","title":"Update APIC with new release","text":"<p>When there is new release of APIC, you can update the existing registry. For example, the registry was built with <code>CASE_NAME=ibm-apiconnect</code> &amp; <code>CASE_VERSION=4.0.2</code>.</p> <ul> <li> <p>Tweak the environment variables    Set <code>CASE_VERSION=4.0.3</code>. The other environment variables stay the same. Verify values with:  <pre><code># echo $CASE_NAME\nibm-apiconnect\n# echo $CASE_VERSION\n4.0.3\n</code></pre></p> </li> <li> <p>Download the API Connect image inventory <pre><code>oc ibm-pak get $CASE_NAME \\\n  --version $CASE_VERSION  \n</code></pre></p> </li> <li> <p>Generate mirror manifests   <pre><code>oc ibm-pak generate mirror-manifests \\\n  $CASE_NAME $TARGET_REGISTRY --version $CASE_VERSION  \n</code></pre></p> </li> <li> <p>Mirror images to the TARGET_REGISTRY  </p> <p>Note: We presume that credentials and entitlement keys have not changed. <pre><code>export REGISTRY_AUTH_FILE=/opt/registry/auth/cp-auth.json  \n</code></pre>   Use the <code>nohup oc image mirror</code> command appropriate for your insecure or secure bastion registry from  Populate bastion with images</p> </li> <li> <p>Image Content Source Policy (ICSP)    Apply the new ICSP (might not have changed).     <pre><code>oc apply -f  \\\n  ~/.ibm-pak/data/mirror/$CASE_NAME/$CASE_VERSION/image-content-source-policy.yaml  \n</code></pre></p> </li> <li> <p>Install Catalog Sources  </p> <p>Note:  Examine <code>~/.ibm-pak/data/mirror/${CASE_NAME}/${CASE_VERSION}</code> to determine if there any \\$ARCH specific <code>catalog-sources</code> files.  </p> <ul> <li> <p>Apply catalog sources <pre><code>oc apply -f ~/.ibm-pak/data/mirror/${CASE_NAME}/${CASE_VERSION}/catalog-sources.yaml\n</code></pre></p> </li> <li> <p>Apply \\$ARCH specific catalog sources <pre><code>oc apply -f \\\n  ~/.ibm-pak/data/mirror/${CASE_NAME}/${CASE_VERSION}/catalog-sources-linux-${ARCH}.yaml\n</code></pre></p> </li> <li>List catalog sources defined in OCP: <pre><code>oc get catalogsource -n openshift-marketplace\n</code></pre></li> </ul> </li> <li> <p>Update APIC on OCP web console   Subscribe to APIC Operator Channel v3.2 and Operator Version 3.2.1, which corresponds to CASE_VERSION 4.0.3. Reference: Operator, operand, and CASE versions.</p> </li> <li>Follow upgrade steps in the IBM API Connect document  </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#rebuild-bastion-repository","title":"Rebuild bastion repository","text":"<p>In case you need to rebuild the bastion repository, follow the steps below to build a replica of the images in the old registry.  </p>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#prerequisites","title":"Prerequisites","text":"<ul> <li>Setup OS &amp; OCP/CP4I bastion tools Configure Linux Bastion</li> <li>Use the same values for:   CASE_NAME   CASE_VERSION   ARCH  </li> <li> <p>Files from the old bastion   Assuming the old bastion was built with <code>ibm-pak</code>:  </p> <ul> <li>Config    Transfer the file <code>.ibm-pak/config/config.yaml</code> to the same path in the new bastion.  </li> <li>CASE files   Transfer files in <code>~/.ibm-pak/data/cases</code> to the new bastion using the same directory path.  </li> </ul> <p>Note: Backup <code>~/.ibm-pak</code> as safety net.</p> </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#build-the-new-bastion","title":"Build the new bastion","text":"<ul> <li>Create the podman registry   Follow the recipe in Create your private container registry.   </li> <li>Mirror images to your private container registry <ul> <li>Section Setup ibm-pak   Start at Generate mirror manifests .    <p>Note: The prior steps, especially <code>oc ibm-pak get $CASE_NAME --version $CASE_VERSION</code> will create a new manifest which is likely to lead to a different set of images, not a replica of the old bastion.  </p> </li> <li>Section Populate bastion with images    Run all steps.  </li> <li>Section Configure the OCP cluster   Run all steps.  </li> </ul> </li> <li>Verify installation   IBM API Connect, DataPower &amp; Common (Foundational) Services should be unaffected as the podman registry contains the same imagess as the old bastion.  </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#call-outs-for-secure-registry","title":"Call Outs for Secure Registry","text":""},{"location":"dev-ops/bastion-airgap-ibm-pak/#create-a-secure-container-registry","title":"Create a secure container registry","text":"<p>If your enterprise requires secure communications, you should obtain TLS .crt &amp; .key files appropriate for the bastion host. Self signed certificates do not result in secure registries unless you define trusts on the bastion server and on OCP. This document provides steps to create and use self signed TLS. For corporate TLS, you might not need to prime trust settings on the bastion server.    </p> <p>Please see How to implement a simple personal/private Linux container image registry for internal use section Start\u00a0the registry for details. This document details steps to create and use self signed TLS.    </p> <p>If you really wish to use self signed certificates, see Setting up additional trusted certificate authorities for builds .  </p>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#myca-authority","title":"MyCA Authority","text":"<ul> <li>Generate the private key <code>openssl genrsa -out my-ca-privkey.pem 2048</code> </li> <li>Create MyCA certificate      The subject parameters are in ca-cert.conf <pre><code>openssl req -new -x509 -days 3650 -key my-ca-privkey.pem \\\n  -out my-ca-cert.pem -config ca-cert.conf \n</code></pre></li> <li>Make MyCA a trusted authority on bastion server <pre><code>cp my-ca-cert.pem /etc/pki/ca-trust/source/anchors/ \nupdate-ca-trust  \n</code></pre></li> <li>Verify #1 <pre><code>trust list --filter=ca-anchors | grep Ravi -i -A 2 -B 3\n</code></pre> <pre><code>pkcs11:id=%1D%4C%1C%66%06%A7%2D%D7%9B%B9%C9%90%4B%3A%3E%0D%62%75%E3%F6;type=cert\n    type: certificate\n    label: Ravi CA\n    trust: anchor\n    category: authority\n</code></pre></li> <li>Verify #2 <pre><code>awk -v cmd='openssl x509 -noout -subject' '/BEGIN/{close(cmd)};{print | cmd}' \\\n  &lt; /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem | grep Ravi\n</code></pre> <pre><code>Could not read certificate from &lt;stdin&gt;\nUnable to load certificate\nsubject=C = US, ST = NY, L = Ether, O = MyCA, OU = MyCAOrg, CN = Ravi CA  \n</code></pre></li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#bastion-tls-key-cert","title":"Bastion TLS key &amp; cert","text":"<ul> <li>Generate the private key &amp; certificate signing request     The subject parameters are in bastion.conf:  <pre><code>openssl genrsa -out bastion-privkey.pem 2048  \nopenssl req -new -key bastion-privkey.pem -out bastion.csr -config bastion.conf \n</code></pre></li> <li>Create the bastion certificate <code>openssl x509 -req -in bastion.csr -CA my-ca-cert.pem -CAkey my-ca-privkey.pem \\       -out bastion-cert.pem -CAcreateserial -days 265 -sha256 \\       -extensions v3_req -extfile bastion.conf</code></li> <li> <p>Examine the bastion certificate <pre><code>openssl x509 -noout -text -in bastion-cert.pem   \n</code></pre>     An alternate approach to examine Subject Alternate Name (SAN): <pre><code>openssl x509 -text -noout -certopt \\\n  no_subject,no_header,no_version,no_serial,no_signame,no_validity\\\n  -certopt no_issuer,no_pubkey,no_sigdump,no_aux \\\n  -ext subjectAltName -in bastion-cert.pem   \n</code></pre> <pre><code>        X509v3 extensions:\n            X509v3 Subject Alternative Name: \n                DNS:onyx1.myco.com\nX509v3 Subject Alternative Name: \n    DNS:onyx1.myco.com\n</code></pre></p> </li> <li> <p>Place the files in <code>/opt/registry/certs/</code> <pre><code>cp bastion-privkey.pem /opt/registry/certs/\ncp bastion-cert.pem /opt/registry/certs/\n</code></pre></p> </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#start-the-secure-registry","title":"Start the secure registry","text":"<pre><code>podman run -d --name apic-registry -p 5000:5000 \\\n  -v /opt/registry/data:/var/lib/registry \\\n  -v /opt/registry/auth:/auth \\\n  -e \"REGISTRY_AUTH=htpasswd\" \\\n  -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\\n  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\\n  -v /opt/registry/certs:/certs:z \\\n  -e \"REGISTRY_HTTP_TLS_CERTIFICATE=/certs/bastion-cert.pem\" \\\n  -e \"REGISTRY_HTTP_TLS_KEY=/certs/bastion-privkey.pem\" \\\n  --restart=always \\\n  docker.io/library/registry:2  \n</code></pre>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#verify-the-registrys-certificate","title":"Verify the registry's certificate","text":"<pre><code># echo | openssl s_client -connect $TARGET_REGISTRY -servername `hostname -f` \\\n  | grep -B6 \"Verification\"  \ndepth=1 C = US, ST = NY, L = Ether, O = MyCA, OU = MyCAOrg, CN = Ravi CA\nverify return:1\ndepth=0 C = US, ST = FL, L = Gainesville, O = MyCo, OU = APIC, CN = onyx1.myco.com\nverify return:1\nNo client certificate CA names sent\nPeer signing digest: SHA256\nPeer signature type: RSA-PSS\nServer Temp Key: X25519, 253 bits\n\nSSL handshake has read 1391 bytes and written 386 bytes\nDONE\nVerification: OK\n</code></pre>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#verify-access-to-secure-registry","title":"Verify access to secure registry","text":"<p>Note: Do not use the <code>-k</code> option to bypass TLS.  </p> <pre><code># curl --user &lt;username:pswd&gt; https://$TARGET_REGISTRY/v2/_catalog\n{\"repositories\":[]}\n</code></pre> <p>Return to Call Out 1 </p>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#connect-your-secure-registry-to-ocp","title":"Connect your secure registry to OCP","text":""},{"location":"dev-ops/bastion-airgap-ibm-pak/#secure-registry","title":"Secure Registry","text":"<p>You need to provide OCP with TLS credentials to trust the bastion image registry. Setting up additional trusted certificate authorities for builds provides the steps.  </p> <ul> <li>Create a ConfigMap in the <code>openshift-config</code> namespace containing the trusted certificates for the registries that use self-signed certificates. For each CA file, ensure the key in the ConfigMap is the hostname of the registry in the <code>hostname[..port]</code> format: <pre><code>oc create configmap registry-cas -n openshift-config \\\n--from-file=`hostname -f`..5000=/etc/pki/ca-trust/source/anchors/my-ca-cert.pem \n</code></pre></li> </ul> <p>If you have multiple registries, place them all in <code>registry-cas</code> with multiple <code>--from-file</code> entries.  </p> <ul> <li> <p>Modify in the cluster image configuration in <code>image.config.openshift.io/cluster</code>:       Recommend editing the <code>image.config.openshift.io/cluster</code> manually through the OCP GUI or CLI: <code>oc edit image.config.openshift.io/cluster</code> <pre><code>spec:\n  additionalTrustedCA:\n    name: registry-cas\n  allowedRegistriesForImport:\n    - domainName: '&lt;$TARGET_REGISTRY&gt;'\n      insecure: false\n</code></pre>     Define additional <code>- domainName</code> entries for each secure registry.  </p> <p>The image configuration entries in your installation might contain more entries. For an example, see <code>image.config.openshift.io/cluster CR</code> in Procedure Item 1 of 9.2.4. Adding registries that allow image short names</p> </li> </ul> <p>Return to Call Out 2</p>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#references","title":"References","text":""},{"location":"dev-ops/bastion-airgap-ibm-pak/#docker-podman-registries","title":"Docker / Podman Registries","text":"<ul> <li>How to implement a simple personal/private Linux container image registry for internal use    Explains <code>podman</code> command to create a secure image registry. If you have a corporate TLS,this is all you need. If you need to create self signed TLS, there are other reference documents.  </li> <li>Setting up additional trusted certificate authorities for builds     Provides steps to define TLS trust in OCP whether you have a corporate or self signed TLS.  </li> <li>htpasswd - Manage user files for basic authentication </li> </ul>"},{"location":"dev-ops/bastion-airgap-ibm-pak/#general-interest","title":"General Interest","text":"<ul> <li>Is your Operator Air-Gap Friendly?</li> <li>Disconnected installation mirroring</li> <li>Making CA certificates available to Linux command-line tools</li> <li>SSL Certificate Verification</li> <li>Add X.509 extensions to Certificate Signing Request (CSR)</li> <li>Using SSL to protect connections to Red Hat Quay</li> </ul>"},{"location":"dev-ops/create-oidc-users-from-ldap/","title":"Create LDAP users in OIDC","text":"<p>Sila Kissuu  \u00a9 IBM v0.1  2024-03-25   </p> <ol> <li> <p>Save LDAP/LUR registry users for each Provider Organization to a file:</p> <pre><code>apic users:list -o {yourProviderOrg} -s {yourMgmtServer}   \n--user-registry {yourRegistry} --fields username,email,first_name,last_name   \n--format json &gt; ldap-users.json\n</code></pre> <p>TIP: </p> <p>Adjust path to \"ldap-users.json:\" as needed  </p> <p>Use the <code>apic user-registries:list</code> command to get a list of registry names in your Provider Org.</p> </li> <li> <p>File contents from Step 1:</p> <p><pre><code>  1 {\n  2     \"total_results\": 2,\n  3     \"results\": [\n  4         {\n  5             \"username\": \"user1@ibm.com\",\n  6             \"email\": \"user1@ibm.com\",\n  7             \"first_name\": \"User\",\n  8             \"last_name\": \"One\"\n  9         },\n 10         {\n 11             \"username\": \"user2@ibm.com\",\n 12             \"email\": \"user2@ibm.com\",\n 13             \"first_name\": \"User\",\n 14             \"last_name\": \"Two\"\n 15         }\n 16     ]\n 17 }\n</code></pre> Note down the value of \"total_results\". We will need it for verification purposes in Step 6. </p> </li> <li> <p>Remove 4 extra elements so that you are left with an array of users. </p> <ul> <li>Delete line 1 </li> <li>Delete line 2 </li> <li>Remove the string '\u201cresults\u201d:' from line 3</li> <li>Delete the last line - line 17 in this example.   </li> </ul> </li> <li> <p>The updated file will look like this:</p> <pre><code>[\n  {\n    \"username\": \"user1@ibm.com\",\n    \"email\": \"user1@ibm.com\",\n    \"first_name\": \"User\",\n    \"last_name\": \"One\"\n  },\n  {\n    \"username\": \"user2@ibm.com\",\n    \"email\": \"user2@ibm.com\",\n    \"first_name\": \"User\",\n    \"last_name\": \"Two\"\n  }\n]\n</code></pre> </li> <li> <p>Here is python code that reads the file containing an array of multiple users and creates a separate JSON file for each user in the array. Why? The user:create command accepts a file with only 1 user in it.</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nCreated on Mon Mar 25 11:53:25 2024\n\n@author: sila\n\"\"\"\nimport json\nimport os\n\nwith open('/{pathTo}/ldap-users.json', 'r') as file:\n    users = json.load(file)\n\noutput_dir = '/{pathTo}/output_files'    \nos.makedirs(output_dir, exist_ok=True)\n\n# Iterate over the array and process each user individually\nfor user in users:\n    # Extract email for the filename\n    filename = user[\"email\"].lower() + \".json\"\n\n    # Write each user to a separate file\n    with open(os.path.join(output_dir, filename), 'w') as file:\n        json.dump(user, file, indent=4)\n</code></pre> </li> <li> <p>Verify files were written. Number of files should equal number of users in the file obtained in Step 2 - see the value of the element \"total_results\" which in this case is 2.  </p> <p><code>tree -f output_files</code> <pre><code>output_files\n\u251c\u2500\u2500 output_files/user1@ibm.com.json\n\u2514\u2500\u2500 output_files/user2@ibm.com.json\n\n1 directory, 2 files\n</code></pre></p> </li> <li> <p>Verify file contents. <code>cat output_files/user2@ibm.com.json</code></p> <pre><code>{\n    \"username\": \"user2@ibm.com\",\n    \"email\": \"user2@ibm.com\",\n    \"first_name\": \"User\",\n    \"last_name\": \"Two\"\n}\n</code></pre> </li> <li> <p>You can create each user in OIDC by passing their corresponding individual file to the users:create command. <pre><code>apic users:create -o {yourProviderOrg} -s {yourMgmtServer}  \n --user-registry {yourOIDCRegistry} output_files/user2@ibm.com.json\n</code></pre></p> <pre><code>user2-ibm.com    [state: enabled]   https://{yourMgmtServer}/api/user-registries/86441fe3-dfed-4fe6-99ef-6153b0d14afe/7311fdd9-8cee-4a34-8fdc-398ae61f9426/users/91e3a911-e687-4ae6-8867-0b31cbb85d04\n</code></pre> </li> <li> <p>Verify user exists: <code>apic users:list -o {yourProviderOrg} -s {yourMgmtServer} --user-registry {yourOIDCRegistry} | cut -d' ' -f1 | sed 's/-/@/'</code> </p> <pre><code>user1@ibm.com\nuser2@ibm.com\n</code></pre> </li> <li> <p>You can loop through the files in the output directory to create all users in one users:create command. <code>for file in *; do echo \"creating user \" $file; apic users:create -o {yourProviderOrg} -s {yourMgmtServer} --user-registry {yourOIDCRegistry}$file; done 2&gt;/tmp/error.log</code> <pre><code>creating user  user1@ibm.com.json\nuser1-ibm.com    [state: enabled]   https://{yourMgmtServer}/api/user-registries/86441fe3-dfed-4fe6-99ef-6153b0d14afe/7311fdd9-8cee-4a34-8fdc-398ae61f9426/users/00942444-66cd-463d-9a49-44c3223f426e\ncreating user  user2@ibm.com.json\nuser2-ibm.com    [state: enabled]   https://{yourMgmtServer}/api/user-registries/86441fe3-dfed-4fe6-99ef-6153b0d14afe/7311fdd9-8cee-4a34-8fdc-398ae61f9426/users/a0cb650a-d5b3-4929-8971-77606f9f90e3\n</code></pre></p> <p>Note: errors encountered during user creation will be written to /tmp/error.log (adjust file path as needed).</p> <p>For example, attempts to create a user that already exists will result in the following:</p> <p><code>cat /tmp/error.log</code> <pre><code>Error: The user with username user1@ibm.com already exists in the {yourOIDCRegistry} identity provider.\nError: The user with username user2@ibm.com already exists in the {yourOIDCRegistry} identity provider.\n</code></pre></p> </li> </ol>"},{"location":"dev-ops/create-oidc-users-from-ldap/#references","title":"References:","text":"<ul> <li>APIC Toolkit Reference</li> <li>Migrate users from LUR to OIDC - APIC v10 </li> </ul>"},{"location":"dev-ops/global-error-policy/","title":"Global Error Policy: Mask Sensitive Data","text":"<p>Sila Kissuu  \u00a9 IBM v0.1  2024-05-24   </p> <p>We demonstrate how to create and designate a global error policy. In this policy we mask the IP address returned in the X-Client-Ip response header.</p>"},{"location":"dev-ops/global-error-policy/#procedure","title":"PROCEDURE","text":"<ol> <li> <p>Create a global policy yaml file.</p> <p>Contents of the global_error_policy.yaml used in this document:</p> <pre><code>global-policy: 1.0.0\ninfo:\n  name: mask-ip-error-policy\n  title: Mask IP Error Policy\n  version: 2.0.0\ngateways:\n  - datapower-api-gateway\nassembly:\n  catch:\n    - errors:\n        - AssemblyRateLimitError\n        - RuntimeError\n      execute:\n        - log:\n            title: log\n            log-level: default\n            mode: gather-only\n            version: 2.1.0\n    - default:\n        - log:\n            title: log\n            log-level: default\n            mode: gather-only\n            version: 2.1.0\n\n  finally:\n    - gatewayscript:\n        version: 2.0.0\n        title: Mask IP Address\n        source: &gt;-\n          context.message.header.set('X-Client-Ip','****');\n          context.message.header.set('X-Forwarded-For','****');\n          context.message.header.set('X-FRAME-OPTIONS','DENY');\n          context.message.header.set('X-XSS-Protection','1; mode=block');\n          console.log('x-client-ip header=',context.message.header.get('X-Client-Ip'));\n          console.log('X-Forwarded-For header=',context.message.header.get('X-Forwarded-For'));\n</code></pre> </li> <li> <p>Login to the Management server using the provider realm.</p> <p><code>apic login --server mgmt_endpoint_url --username user_id --password password --realm provider/identity_provider</code></p> </li> <li> <p>Upload the global error policy to a gateway service.</p> <p><code>apic global-policies:create --catalog sandbox --configured-gateway-service yourGwyService --org yourOrg --server platformEndpointUrl --scope catalog global_error_policy.yaml</code></p> <p>Response: <pre><code>mask-ip-error-policy:2.0.0   https://platform.v10-cd-mgmt.rtp.raleigh.ibm.com/api/catalogs/86441fe3-dfed-4fe6-99ef-6153b0d14afe/7335a813-3082-4c98-998b-a40ebea70abb/configured-gateway-services/9acd7b0a-bc2f-4ace-b3b7-9223e7f91db4/global-policies/7a392a17-a84a-4858-9409-4738c774cfe6\n</code></pre></p> <p>NOTE:   If your yaml is not structured correctly - for example, you used tabs for indentation instead of spaces - you might encounter an error such as this one: <pre><code>Error: The 'global_policy' property must include an 'info' object.\n</code></pre></p> </li> </ol>"},{"location":"dev-ops/global-error-policy/#verification","title":"VERIFICATION","text":"<ol> <li> <p>Verify that the global policy has been successfully uploaded to the gateway service. <code>apic global-policies:list-all --catalog sandbox --configured-gateway-service yourGwyService --org yourOrg --server platformEndpointUrl --scope catalog</code></p> <p>Response: <pre><code>mask-ip-error-policy:2.0.0   https://platform.v10-cd-mgmt.rtp.raleigh.ibm.com/api/catalogs/86441fe3-dfed-4fe6-99ef-6153b0d14afe/7335a813-3082-4c98-998b-a40ebea70abb/configured-gateway-services/9acd7b0a-bc2f-4ace-b3b7-9223e7f91db4/global-policies/7a392a17-a84a-4858-9409-4738c774cfe6\n</code></pre></p> </li> <li> <p>Designate the global error policy for the gateway service.  </p> <ul> <li>Retrieve the URL of the policy. <code>apic global-policies:get --catalog sandbox --configured-gateway-service yourGwyService --org yourOrg --server platformEndpointUrl --scope catalog mask-ip-error-policy:2.0.0 --fields url</code> </li> </ul> <p>Response: <pre><code>GlobalPolicy   GlobalPolicy.yaml   https://platform.v10-cd-mgmt.rtp.raleigh.ibm.com/api/catalogs/86441fe3-dfed-4fe6-99ef-6153b0d14afe/7335a813-3082-4c98-998b-a40ebea70abb/configured-gateway-services/9acd7b0a-bc2f-4ace-b3b7-9223e7f91db4/global-policies/7a392a17-a84a-4858-9409-4738c774cfe6\n</code></pre></p> <pre><code>The retrieved URL is witten to a file named `GlobalPolicy.yaml` in the current working directory.\n</code></pre> <ul> <li>Edit the file and replace the word \"url\" with \"global_policy_url\". The contents of our edited <code>GlobalPolicy.yaml</code> are shown below.  </li> </ul> <pre><code>global_policy_url: 'https://platform.v10-cd-mgmt.rtp.raleigh.ibm.com/api/catalogs/86441fe3-dfed-4fe6-99ef-6153b0d14afe/7335a813-3082-4c98-998b-a40ebea70abb/configured-gateway-services/9acd7b0a-bc2f-4ace-b3b7-9223e7f91db4/global-policies/7a392a17-a84a-4858-9409-4738c774cfe6'\n</code></pre> <ul> <li>Designate this policy as the global error policy as shown:</li> </ul> <p><code>apic global-policy-errors:create --catalog sandbox --configured-gateway-service yourGwyService --org yourOrg --server yourAPIMserver --scope catalog GlobalPolicy.yaml</code> </p> <p>Response: </p> <p><pre><code>global-policy-error   https://platform.v10-cd-mgmt.rtp.raleigh.ibm.com/api/catalogs/86441fe3-dfed-4fe6-99ef-6153b0d14afe/7335a813-3082-4c98-998b-a40ebea70abb/configured-gateway-services/9acd7b0a-bc2f-4ace-b3b7-9223e7f91db4/global-policy-error\n</code></pre> If you need to make changes to your policy (for example, due to gatewayscript compilation errors), simply edit you policy file and run the following command to update the gateway service:</p> <p><code>apic global-policies:update --catalog sandbox --configured-gateway-service yourGwyService --org yourOrg --server yourAPIMserver --scope catalog mask-ip-error-policy:2.0.0 global_error_policy.yaml</code> </p> <p>Response: <pre><code>mask-ip-error-policy:2.0.0   https://platform.v10-cd-mgmt.rtp.raleigh.ibm.com/api/catalogs/86441fe3-dfed-4fe6-99ef-6153b0d14afe/7335a813-3082-4c98-998b-a40ebea70abb/configured-gateway-services/9acd7b0a-bc2f-4ace-b3b7-9223e7f91db4/global-policies/7a392a17-a84a-4858-9409-4738c774cfe6\n</code></pre></p> </li> </ol> <p>You do not need to re-designate the policy when making updates to an already designated policy.</p>"},{"location":"dev-ops/global-error-policy/#test-the-global-error-policy","title":"Test the global error policy","text":"<p>Happy Path (HTTP 200)</p> <p> </p> <p>Error Path (HTTP 404) </p> <p>Notice the X-Client-IP header is masked in both success and error scenarios.  </p>"},{"location":"dev-ops/global-error-policy/#reference","title":"Reference","text":"<ul> <li>APIC v10.0.5.x LTS Documentation: Working with Global Policies</li> </ul>"},{"location":"governance/APIC-Governance-Version/","title":"Governance &amp; Version Control for API Products","text":""},{"location":"governance/APIC-Governance-Version/#ibm-api-connect-governance-models","title":"IBM API Connect: Governance Models","text":"<p>Ravi Ramnarayan  \u00a9 IBM v1.51  2021-04-20    </p>"},{"location":"governance/APIC-Governance-Version/#goals","title":"Goals","text":"<ul> <li>Ensure business continuity while publishing new versions of API Products  </li> <li>Control consumer subscriptions to API Products  </li> <li>Allow API provider teams to develop and test API quickly  </li> </ul>"},{"location":"governance/APIC-Governance-Version/#collateral-effects","title":"Collateral effects","text":"<ul> <li>Deprecate and Retire API Products gracefully  </li> </ul>"},{"location":"governance/APIC-Governance-Version/#api-product-lifecycle","title":"API Product Lifecycle","text":"<p>The diagram below is advertisement to lead you to the core of API management The Product lifecycle.  </p> <p></p> <p>You are familiar with publishing API Products. You may not have looked at Deprecate and Retire while racing towards implementation. In addition to explicit Deprecation, you can do the same implicitly by Superseding a Product with another Product. Similarly, Replacing a Product with another Product will Retire the existing API Product.</p> <p>Note: Replacing a Product with another Product could be a new version of the same Product or a completely different Product.  </p>"},{"location":"governance/APIC-Governance-Version/#api-connect-options","title":"API Connect options","text":"<p>You can configure API Connect (APIC) to suit your needs.  </p>"},{"location":"governance/APIC-Governance-Version/#do-you-want-strict-version-control-for-api-products","title":"Do you want strict version control for API Products?","text":"<p>Creating and configuring Catalogs contains steps to enable Production Mode. Once enabled, you must publish new versions of API Products. You will not be able to publish API Products with the same version number as existing products.  </p> <p></p> <p>Note: You can enable Production Mode for catalogs even in non production environments, if you want to enforce versions for API Products.  </p>"},{"location":"governance/APIC-Governance-Version/#do-you-want-api-providers-to-develop-and-test-api-quickly","title":"Do you want API providers to develop and test API quickly?","text":"<p>If you do not enable Production Mode for a catalog, API developers might find it easier to publish new iterations.  </p> <p> </p> <p>Note: You can disable Production Mode for catalogs even in production installations, if you do not want to enforce versions for API Products.</p>"},{"location":"governance/APIC-Governance-Version/#do-you-want-to-control-consumer-access-to-corporate-resources","title":"Do you want to control consumer access to corporate resources?","text":"<p>Managing the application lifecycle is available in APIC v5. If you have used this feature in APIC v5, you can continue to use it with API Connect v5 compatible (v5c) gateway services.</p> <p></p> <p>Orange API calls from consumer application with approved subscriptions reach the corporate resource. Blue API calls from subscriptions without approvals receive response from a dummy endpoint. You will have to encode logic in the gateway script to test the context variable client.app.lifecycle-state and direct the request to the appropriate endpoint.</p> <p>Note: Though Application Lifecycle is not fully documented for the new API Gateway service, the context variable <code>client.app.lifecycle-state</code> carries <code>PRODUCTION</code> or <code>DEVELOPMENT</code> when Application Lifecyle is enabled for the Catalog.   </p>"},{"location":"governance/APIC-Governance-Version/#versions-for-api-products-api-definitions","title":"Versions for API Products &amp; API definitions","text":""},{"location":"governance/APIC-Governance-Version/#governance-model","title":"Governance Model","text":"<p>The Governance Model recommends operations to effect changes and guidelines to assign versions.  </p> <ul> <li>The Version aspect applies to API Definitions and API Products   If you want to enforce versions you should enable Production Mode for the Catalogs in the Production or PreProd environments. You might want to disable Production Mode in lower environments to allow free rein to API developers.  </li> <li>The Operations aspect applies to API Products   The Operation comprises the publish verb and associated options. You should compose and validate the command line before automating it in a pipeline. The Governance Model does not show an operation for Bug Fix in Non Prod. You might wish to use the operation Replace last deployed similar to Major/Minor Release.  </li> </ul> <p></p> <p>Note: The Governance matrix is an approach which you should modify to suit your needs.  </p> <ul> <li>Major Release: Increment the major segment of the version for API breaking changes such as changes to API endpoints, invocation parameters or payload syntax. For example, change the version from 1.1.4 -&gt; 2.0.0. The new version of the API could be be published to the UAT and PROD catalogs with subscriptions migrated later. Once all the subscriptions are migrated, the older version of the API should be retired. The sequence of operations could be:<ul> <li>Publish v2.0.0  </li> <li>Deprecate v1.1.4   <p>Note: Publish supersede combines publish and deprecate in one operation.</p> </li> <li>Migrate subscriptions from v1.1.4 to v2.0.0  </li> <li>Retire v1.1.4  </li> </ul> </li> </ul> <p>You can send email notices to consumers to keep them informed.  </p> <ul> <li>Minor Release: Increment the minor segment of the version for non-breaking changes such as an optional calling parameter. For example, change the version from 1.0.0 -&gt; 1.1.0. You can replace the old version with the new, migrate subscriptions and retire the old version in single operation.  </li> <li>Bug Fix: For bug fixes and/or patches to the API implementation which do not affect the API interface, you may increment the last segment in the version.  </li> </ul>"},{"location":"governance/APIC-Governance-Version/#api-api-products-versions","title":"API, API Products &amp; Versions","text":"<p>API and API Products have different life cycles. The table below illustrates an approach. You can assign the version numbers according to your organizations practices.</p> API Product 1.0.0 API Product 1.1.0 API Product 2.0.0 Notes API-a 1.0.0 1.0.0 1.0.1 One bug fix change to API-a API-b 1.0.0 1.1.0 1.1.0 One minor change to API-b API-c 1.1.0 1.2.0 2.0.0 Minor &amp; major changes to API-c API-d n/a n/a 3.1.0 API-d is new to Product <p>API Version API Definitions exist within a Provider Organization. See API Connect concepts. API Connect allows you to have multiple versions of the same API which is typical during transitions. During steady states, you should try to have only one version of an API within a Provider Organization.  </p> <p>Note: API and API Definition are synonymous. API belong to a Provider Organization and could be used in zero or many API Products.  </p>"},{"location":"governance/APIC-Governance-Version/#should-the-api-uri-contain-the-version-number","title":"Should the API URI contain the version number?","text":"<p>API Connect allows you to implement API URI according to your corporate standards. There are several articles on this topic: Some Yes, some No-No and a few fence straddlers. A small sample of articles:</p> <ul> <li>Best Practices for Versioning REST APIs - Better Programming - Medium </li> <li>REST API versioning for Azure DevOps Services (and TFS) - Azure DevOps | Microsoft Docs </li> <li>REST API Versioning - Is There a Right Answer? - DZone Integration </li> <li>rest - Best practices for API versioning? - Stack Overflow </li> </ul>"},{"location":"governance/APIC-Governance-Version/#i-do-not-like-version-numbers-in-the-api-uri-because-my-consumers-will-be-forced-to-make-changes-when-i-bump-the-version-number-what-can-api-connect-do-for-me","title":"I do not like version numbers in the API URI because my consumers will be forced to make changes when I bump the version number. What can API Connect do for me?","text":"<p>API Connect helps you provide a smooth experience for your consumers. You can implement API URI without version numbers and publish new versions as long as you require consumers to subscribe to plans. API Connect will direct the consumer invocations based on the Subscription ClientId. Consider the following scenario:  </p> <ul> <li>api:v1 &amp; api:v2 share the same API name and URI (base path) which does not contain the version number  </li> <li>api:v1 is in product product:v1, api:v2 is in product product:v2 </li> <li>both products are in the same Provider Organization &amp; Catalog  </li> <li>both products are active and require subscriptions to plans  </li> <li>both api:v1 &amp; api:v2 require ClientId in the HTTP header  </li> <li>consumer-app-1 subscribes to product:v1-plan, consumer-app-2 subscribes to product:v2-plan </li> </ul> <p>Q: Will calls from consumer-1 reach api:v1 &amp; calls from consumer-2 reach api:v2 ?   A: Yes &amp; Yes. Q: Really, even though the plan names are the same? A: Yes. api:v1 &amp; api:v2 have different Client ID's which are tied to different subscriptions.  </p> <p>Yes, this is hard to believe. I had to prove it to myself. Should you? - Clone api:v1 to api:v2   Invoke a back end service which delivers a different result - Clone product:v1 to product:v2    Relate product:v2 to api:v2 - Publish both products to the same Gateway Service    </p> <p>Note: Both products offer the same Paths and Operations  </p> <ul> <li>Subscribe consumer-app-1 to product:v1-plan </li> <li>Subscribe consumer-app-2 to product:v2-plan </li> <li>Invoke the each API with the appropriate app Client ID   Do you see results from two different back end services?   <p>Note: If you clone (copy) api:v1 to api:v2 and use the API Designer Test feature, calls will go to api:v1 as the Client ID is still the same. To avoid this problem, publish api:v2, create a new consumer application and use an external tool such as Postman with the new consumer app Client ID.  </p> </li> </ul>"},{"location":"governance/APIC-Governance-Version/#lifecycle-mechanics","title":"Lifecycle mechanics","text":"<p>Let's walk through a few scenarios and illustrate commands which can be used for DevOps in Production. The scenarios below traverse Production and Staging environments. You are likely to have different settings in each environment. For example, credentials and back end services are likely to be different in Production and lower installations. You should ensure the promotion processes accommodate the differences.  </p>"},{"location":"governance/APIC-Governance-Version/#scenario-a","title":"Scenario A","text":"Env Catalogs API Products API Definitions Subscriptions Production ProdMode-Enabled Version Reqd Version Reqd Required Staging ProdMode-Enabled Version Reqd Version Reqd Required Development ProdMode-Disabled Version Opt. Version Opt. Optional"},{"location":"governance/APIC-Governance-Version/#pro","title":"Pro","text":"<ul> <li>Enforces version control for API Products &amp; Definitions in Production  </li> <li>Enables rate limits based on Subscription ClientId in Production   </li> <li>Allows API developers freedom to create, publish and test API quickly  </li> </ul>"},{"location":"governance/APIC-Governance-Version/#contra","title":"Contra","text":"<ul> <li>Version control for API Definitions and API Products is confusing    API Products &amp; Definitions travel on separate tracks which can intersect. For example, you might create a new version of an API Definition. When you include the new API version in an existing Product, you should assign a new version to the API Product.  </li> <li>Assigning version numbers is tricky   API developer teams have to reach consensus on the practice of assigning version numbers.  </li> </ul>"},{"location":"governance/APIC-Governance-Version/#scenario-b","title":"Scenario B","text":"Env Catalogs API Products API Definitions Subscriptions Production ProdMode-Disabled Version Opt. Version Opt. Required Staging ProdMode-Disabled Version Opt. Version Opt. Required Development ProdMode-Disabled Version Opt. Version Opt. Optional"},{"location":"governance/APIC-Governance-Version/#pro_1","title":"Pro","text":"<ul> <li>Enables rate limits based on Subscription ClientId in Production   </li> <li>Allows API developers freedom to create, publish and test API quickly in all environments including Production  </li> </ul>"},{"location":"governance/APIC-Governance-Version/#contra_1","title":"Contra","text":"<ul> <li>Does not enforce version control for API Products &amp; Definitions in Production   Lack of version control can lead to problems. The situation can become complex if some API developer teams decide to use versions.  </li> </ul>"},{"location":"governance/APIC-Governance-Version/#scenario-c","title":"Scenario C","text":"Env Catalogs API Products API Definitions Subscriptions Production ProdMode-Disabled Version Opt. Version Opt. Optional Staging ProdMode-Disabled Version Opt. Version Opt. Optional Development ProdMode-Disabled Version Opt. Version Opt. Optional"},{"location":"governance/APIC-Governance-Version/#pro_2","title":"Pro","text":"<ul> <li>Allows API developers freedom to create, publish and test API quickly in all environments including Production  </li> </ul>"},{"location":"governance/APIC-Governance-Version/#contra_2","title":"Contra","text":"<ul> <li>Does not enforce version control for API Products &amp; Definitions in Production   Lack of version control can lead to problems. The situation can become complex if some API developer teams decide to use versions.  </li> <li>Cannot apply rate limits based on Subscription ClientId in Production    You could enforce rate limits through other features of DataPower.  </li> </ul>"},{"location":"governance/APIC-Governance-Version/#summary","title":"Summary","text":"<ul> <li>IBM API Connect supports the API Product Lifecyle and helps you tailor different Governance Models for Production &amp; non production installations.  </li> <li>IBM API Connect gives you the option to design your API without embedding the version number in the URI. You could include the version number in the URI if it suits your business practice.  </li> </ul>"},{"location":"governance/APIC-Governance-Version/#next-step-devops","title":"Next Step -- DevOps","text":"<p>In a future article, we will present how you could manage API Products, Plans and Consumer Subscriptions at scale using DevOps.</p>"},{"location":"governance/APIC-Governance-in-APIM/","title":"APIC Governance in APIM","text":""},{"location":"governance/APIC-Governance-in-APIM/#references","title":"References","text":"<ul> <li>Configuring Governance in APIM</li> <li>API Connect's Native API Governance capability - Part 1</li> </ul>"}]}